{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las familias de modelos codificadores, tales como BERT, ALBERT, DistilBERT, ELECTRA, RoBERTA, y MPnet, han emergido como pilares fundamentales en el campo de procesamiento de lenguaje natural. Estos modelos comparten la arquitectura de transformer y han sido preentrenados para aprender representaciones semánticas profundas del lenguaje.\n",
    "\n",
    "Comencemos analizando BERT, un pionero en la atención bidireccional. BERT, en sus múltiples variantes, como BERT-base y BERT-large, destaca por su capacidad para capturar contextos ricos y relaciones sintácticas. Por otro lado, ALBERT introduce la innovación de la factorización de la matriz de atención, optimizando así la eficiencia computacional sin comprometer el rendimiento. La comparación entre BERT y ALBERT revela el equilibrio entre complejidad y eficiencia.\n",
    "\n",
    "DistilBERT, por su parte, busca la simplicidad sin sacrificar la capacidad representativa. Al destilar el conocimiento de un modelo más grande como BERT, logra una reducción significativa en los parámetros, haciéndolo más liviano y ágil. Esta característica lo posiciona como una opción atractiva en tareas con recursos computacionales limitados.\n",
    "\n",
    "ELECTRA, con su aproximación novedosa, implementa un enfoque de sustitución de palabras para el entrenamiento discriminatorio. Al destacar la eficacia de este método, ELECTRA demuestra ser competitivo en la obtención de representaciones de alta calidad.\n",
    "\n",
    "RoBERTa, una mejora de BERT, ajusta la metodología de preentrenamiento al eliminar la tarea de predicción de la orientación de la siguiente palabra. Esto conduce a representaciones más coherentes y robustas, destacando su eficacia en diversos contextos lingüísticos.\n",
    "\n",
    "Finalmente, MPnet se destaca por su enfoque en la modelación de patrones a nivel de múltiples posiciones. Su capacidad para capturar información contextual a lo largo de múltiples niveles jerárquicos proporciona una perspectiva única en la representación del lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)bert-base-cased/resolve/main/config.json: 100%|██████████| 465/465 [00:00<00:00, 466kB/s]\n",
      "d:\\ProgrammingWindows\\Anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\santi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 263M/263M [01:45<00:00, 2.49MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)cased/resolve/main/tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<?, ?B/s]\n",
      "(…)ilbert-base-cased/resolve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 986kB/s] \n",
      "(…)t-base-cased/resolve/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 824kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\") # convenient! Defaults to Fast\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"I dub thee the unforgiven\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   146, 23700, 20021,  1103,  8362, 14467, 10805,  2109,  1179,\n",
      "           102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1662, -0.1271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "\n",
      "Distribution over labels: tensor([[0.5728, 0.4272]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Option 1\n",
    "model_outputs = model(input_ids=model_inputs.input_ids, attention_mask=model_inputs.attention_mask)\n",
    "\n",
    "# Option 2 - the keys of the dictionary the tokenizer returns are the same as the keyword arguments\n",
    "#            the model expects\n",
    "\n",
    "# f({k1: v1, k2: v2}) = f(k1=v1, k2=v2)\n",
    "\n",
    "model_outputs = model(**model_inputs)\n",
    "\n",
    "\n",
    "\n",
    "print(model_inputs)\n",
    "print()\n",
    "print(model_outputs)\n",
    "print()\n",
    "print(f\"Distribution over labels: {torch.softmax(model_outputs.logits, dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state size (per layer):   torch.Size([1, 11, 768])\n",
      "Attention head size (per layer): torch.Size([1, 12, 11, 11])\n",
      "BaseModelOutput(last_hidden_state=tensor([[[ 0.3658, -0.1363, -0.0241,  ..., -0.0243,  0.2092, -0.0434],\n",
      "         [ 0.3351, -0.4731,  0.4455,  ...,  0.3333,  0.0894,  0.1752],\n",
      "         [ 0.0559, -0.2744,  0.2439,  ...,  0.3805, -0.1535,  0.0684],\n",
      "         ...,\n",
      "         [-0.1702,  0.1649, -0.1826,  ...,  0.7063, -0.4539, -0.0200],\n",
      "         [ 0.1001, -0.6128,  0.0116,  ...,  0.1753,  0.5835,  0.0543],\n",
      "         [ 0.6431, -0.7274, -0.2860,  ...,  0.2709,  0.1994, -0.3916]]]), hidden_states=(tensor([[[ 5.5207e-01,  1.7780e-01, -5.8549e-02,  ..., -1.5978e-02,\n",
      "           2.0846e-01, -1.1543e-01],\n",
      "         [-9.8524e-01,  2.7473e-01,  1.2517e-03,  ...,  1.3920e+00,\n",
      "          -1.1383e+00,  5.2962e-01],\n",
      "         [-7.2930e-01, -2.7470e-02, -6.9238e-01,  ..., -3.3046e-01,\n",
      "          -2.7515e-01, -1.2525e-01],\n",
      "         ...,\n",
      "         [ 1.0097e+00,  7.3380e-01, -1.2838e+00,  ..., -2.1808e-01,\n",
      "           7.8230e-02,  3.1925e-01],\n",
      "         [-4.3315e-01, -2.7047e-02, -3.3336e-01,  ..., -2.6688e-01,\n",
      "           6.2738e-01,  1.9109e-01],\n",
      "         [-8.5291e-02, -3.5893e-02,  2.5175e-01,  ...,  6.9999e-01,\n",
      "          -1.2797e+00,  9.0846e-02]]]), tensor([[[ 0.2952,  0.0513, -0.1022,  ...,  0.0386, -0.0405, -0.0765],\n",
      "         [-0.8180, -0.4604,  0.5049,  ...,  0.8140, -0.5794,  0.3106],\n",
      "         [-1.0249, -0.7582, -0.4621,  ..., -0.3392, -0.1505, -0.0558],\n",
      "         ...,\n",
      "         [ 0.8839,  0.2742, -0.9685,  ...,  0.5019, -0.5540, -0.1136],\n",
      "         [ 0.2059, -0.8111,  0.1264,  ..., -0.3763,  0.3910, -0.1860],\n",
      "         [-0.1016, -0.1486,  0.0918,  ...,  0.1362, -0.3403,  0.0248]]]), tensor([[[ 0.4616,  0.1149, -0.2102,  ..., -0.0875,  0.1470, -0.0661],\n",
      "         [-0.6726, -0.4533,  0.9057,  ...,  0.5372, -0.3243,  0.1406],\n",
      "         [-1.1157, -0.9821, -0.6424,  ..., -0.3677,  0.0029,  0.7136],\n",
      "         ...,\n",
      "         [ 0.7083,  0.3763, -0.6668,  ...,  0.2950, -0.3659, -0.7926],\n",
      "         [-0.2743, -1.1257, -0.3068,  ..., -0.3547,  0.6665,  0.2089],\n",
      "         [-0.0629, -0.1204, -0.0047,  ..., -0.0361, -0.0827, -0.0643]]]), tensor([[[-0.4707,  0.0417, -0.2710,  ...,  0.1936, -0.0819, -0.0866],\n",
      "         [-0.6544, -0.4051,  0.7706,  ...,  0.7493, -0.4737,  0.2015],\n",
      "         [-1.3582, -0.5838, -0.3350,  ...,  0.2002, -0.0475,  0.9204],\n",
      "         ...,\n",
      "         [ 0.1550, -0.0341, -0.2531,  ...,  0.4648, -0.6672, -0.3113],\n",
      "         [-0.3896, -1.3917, -0.1518,  ..., -0.4680,  0.6962,  0.2415],\n",
      "         [-0.0329, -0.0705,  0.0296,  ..., -0.0392, -0.0789, -0.0956]]]), tensor([[[-5.3871e-01,  2.8279e-02, -5.3130e-01,  ...,  2.7295e-01,\n",
      "           7.4318e-02, -1.2443e-01],\n",
      "         [-5.3242e-01, -3.7295e-01,  6.1731e-01,  ...,  5.9131e-01,\n",
      "          -1.7153e-01,  5.0548e-01],\n",
      "         [-1.4471e+00, -6.4107e-01,  2.7364e-01,  ...,  5.7157e-01,\n",
      "          -2.7676e-01,  4.6478e-01],\n",
      "         ...,\n",
      "         [-3.6457e-01,  3.7507e-01, -3.5316e-02,  ...,  8.4388e-01,\n",
      "          -9.4903e-01, -3.7619e-01],\n",
      "         [-5.5972e-01, -1.3334e+00, -1.0984e-01,  ..., -2.3407e-01,\n",
      "           6.7684e-01,  1.5456e-01],\n",
      "         [-4.7876e-04,  3.7925e-02,  3.3058e-02,  ..., -2.4607e-04,\n",
      "           7.3480e-03, -1.0575e-01]]]), tensor([[[-3.1870e-01, -1.4938e-01, -3.1293e-01,  ...,  4.3440e-01,\n",
      "           2.8242e-01, -1.1890e-02],\n",
      "         [ 1.1469e-01, -7.7900e-01,  9.9046e-01,  ...,  7.2513e-01,\n",
      "           2.9779e-01,  5.0378e-01],\n",
      "         [-7.1105e-01, -6.9291e-01,  5.1371e-01,  ...,  6.6529e-01,\n",
      "          -4.0810e-01,  2.6590e-01],\n",
      "         ...,\n",
      "         [-4.1784e-01,  5.1668e-01,  1.2234e-01,  ...,  1.0945e+00,\n",
      "          -9.8018e-01, -2.3756e-01],\n",
      "         [-2.0255e-01, -1.0062e+00,  1.1498e-01,  ..., -1.9523e-01,\n",
      "           8.4177e-01, -8.3561e-02],\n",
      "         [-4.2658e-02,  7.8940e-02,  3.1988e-02,  ..., -1.9827e-04,\n",
      "          -1.0892e-02, -8.4195e-02]]]), tensor([[[ 0.3658, -0.1363, -0.0241,  ..., -0.0243,  0.2092, -0.0434],\n",
      "         [ 0.3351, -0.4731,  0.4455,  ...,  0.3333,  0.0894,  0.1752],\n",
      "         [ 0.0559, -0.2744,  0.2439,  ...,  0.3805, -0.1535,  0.0684],\n",
      "         ...,\n",
      "         [-0.1702,  0.1649, -0.1826,  ...,  0.7063, -0.4539, -0.0200],\n",
      "         [ 0.1001, -0.6128,  0.0116,  ...,  0.1753,  0.5835,  0.0543],\n",
      "         [ 0.6431, -0.7274, -0.2860,  ...,  0.2709,  0.1994, -0.3916]]])), attentions=(tensor([[[[4.8194e-01, 1.2658e-01, 4.4910e-02,  ..., 4.6064e-02,\n",
      "           3.3543e-02, 8.0068e-03],\n",
      "          [3.2221e-01, 2.3916e-03, 1.3622e-01,  ..., 4.1417e-02,\n",
      "           5.6738e-02, 1.1122e-02],\n",
      "          [3.5608e-01, 3.5014e-02, 1.7378e-01,  ..., 9.3648e-02,\n",
      "           3.5332e-02, 1.0430e-03],\n",
      "          ...,\n",
      "          [4.8994e-01, 5.5211e-02, 3.6992e-02,  ..., 2.5072e-02,\n",
      "           4.7489e-02, 7.3157e-04],\n",
      "          [2.8456e-01, 3.6059e-02, 1.0752e-01,  ..., 5.0385e-02,\n",
      "           1.8153e-02, 1.3851e-03],\n",
      "          [5.7572e-05, 6.6604e-06, 1.6279e-05,  ..., 1.0632e-04,\n",
      "           5.6322e-05, 9.9954e-01]],\n",
      "\n",
      "         [[6.1583e-01, 2.7273e-02, 7.0346e-02,  ..., 3.1872e-02,\n",
      "           1.9636e-02, 6.1977e-02],\n",
      "          [3.0136e-01, 6.5934e-01, 1.5999e-02,  ..., 3.4775e-04,\n",
      "           1.0981e-04, 8.1688e-04],\n",
      "          [5.0100e-01, 3.4332e-01, 3.4590e-02,  ..., 1.2383e-02,\n",
      "           2.7244e-03, 2.4410e-03],\n",
      "          ...,\n",
      "          [9.1972e-03, 1.2849e-03, 1.2116e-02,  ..., 9.6071e-03,\n",
      "           1.8762e-02, 3.3928e-03],\n",
      "          [2.0630e-02, 1.5547e-03, 8.0139e-03,  ..., 1.1916e-01,\n",
      "           2.1142e-02, 1.3735e-01],\n",
      "          [3.6696e-03, 6.6002e-05, 2.9990e-04,  ..., 1.8993e-03,\n",
      "           6.9705e-04, 9.9103e-01]],\n",
      "\n",
      "         [[8.6970e-02, 5.0397e-02, 5.1674e-02,  ..., 1.2651e-01,\n",
      "           1.0520e-01, 5.9707e-02],\n",
      "          [1.5387e-02, 5.8630e-02, 1.8458e-01,  ..., 1.1135e-01,\n",
      "           5.7399e-02, 5.5299e-02],\n",
      "          [3.3062e-01, 2.3038e-02, 4.1146e-02,  ..., 1.5084e-01,\n",
      "           1.0163e-01, 3.5540e-02],\n",
      "          ...,\n",
      "          [1.7544e-01, 2.3505e-03, 1.3888e-01,  ..., 7.9432e-02,\n",
      "           7.9089e-02, 1.3716e-02],\n",
      "          [1.3940e-01, 5.0126e-02, 5.5077e-02,  ..., 1.1377e-01,\n",
      "           1.0840e-01, 6.4980e-02],\n",
      "          [5.8378e-02, 2.4032e-02, 8.7658e-02,  ..., 1.4878e-01,\n",
      "           1.8121e-01, 3.8173e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2.9063e-01, 5.4062e-02, 1.1485e-01,  ..., 7.8932e-02,\n",
      "           8.1689e-02, 6.2990e-02],\n",
      "          [2.2177e-02, 1.4703e-01, 1.6292e-01,  ..., 3.0933e-02,\n",
      "           1.3481e-02, 6.5900e-02],\n",
      "          [5.7622e-01, 4.7476e-02, 2.3179e-03,  ..., 6.0801e-02,\n",
      "           1.4123e-02, 9.0837e-02],\n",
      "          ...,\n",
      "          [5.4029e-01, 3.9180e-02, 5.7276e-02,  ..., 1.0895e-02,\n",
      "           8.0812e-02, 4.9156e-02],\n",
      "          [3.5685e-01, 7.7091e-02, 9.2956e-02,  ..., 4.7447e-02,\n",
      "           2.4081e-02, 1.1195e-01],\n",
      "          [5.0900e-03, 2.5452e-02, 2.0435e-02,  ..., 2.8635e-03,\n",
      "           5.1477e-03, 4.2141e-03]],\n",
      "\n",
      "         [[7.1523e-01, 1.3125e-02, 2.2462e-02,  ..., 5.4837e-03,\n",
      "           4.8494e-03, 1.9221e-01],\n",
      "          [4.3047e-01, 2.2053e-01, 6.8062e-03,  ..., 1.2588e-02,\n",
      "           1.2291e-01, 1.2855e-01],\n",
      "          [3.3867e-01, 6.9584e-03, 5.4395e-01,  ..., 7.0790e-03,\n",
      "           5.9096e-03, 4.6500e-02],\n",
      "          ...,\n",
      "          [2.3757e-01, 8.4962e-03, 1.9369e-02,  ..., 6.3526e-01,\n",
      "           1.3517e-03, 5.1879e-02],\n",
      "          [1.0371e-01, 1.7239e-02, 5.0937e-03,  ..., 1.5495e-03,\n",
      "           7.9036e-01, 4.6920e-02],\n",
      "          [1.0701e-01, 4.7000e-03, 3.1190e-03,  ..., 3.9840e-05,\n",
      "           2.1998e-05, 8.7919e-01]],\n",
      "\n",
      "         [[1.1478e-06, 3.9076e-06, 4.8620e-07,  ..., 3.9877e-07,\n",
      "           3.9400e-06, 9.9985e-01],\n",
      "          [1.1736e-01, 1.2308e-01, 6.1720e-02,  ..., 3.8889e-02,\n",
      "           7.3599e-02, 1.0962e-02],\n",
      "          [4.9634e-01, 2.3815e-02, 4.8012e-02,  ..., 4.7496e-02,\n",
      "           3.8113e-02, 2.1671e-03],\n",
      "          ...,\n",
      "          [3.7501e-01, 8.1938e-03, 1.2360e-01,  ..., 3.4172e-02,\n",
      "           4.4244e-02, 1.2085e-03],\n",
      "          [2.9645e-01, 1.0030e-02, 1.7534e-01,  ..., 2.5828e-02,\n",
      "           1.1402e-02, 2.2307e-03],\n",
      "          [6.9933e-02, 2.9631e-01, 2.6727e-02,  ..., 1.5918e-01,\n",
      "           3.1663e-02, 4.2783e-05]]]]), tensor([[[[7.2167e-01, 4.8450e-02, 2.0547e-02,  ..., 2.6003e-02,\n",
      "           2.9704e-02, 9.1885e-02],\n",
      "          [5.1310e-01, 7.2045e-02, 1.6027e-02,  ..., 9.0107e-03,\n",
      "           2.7181e-02, 2.4853e-01],\n",
      "          [2.6895e-01, 1.4357e-01, 4.2424e-02,  ..., 6.6432e-03,\n",
      "           8.2659e-02, 2.3065e-01],\n",
      "          ...,\n",
      "          [7.1639e-01, 9.6915e-03, 5.5407e-02,  ..., 3.6037e-04,\n",
      "           7.2967e-02, 6.9440e-02],\n",
      "          [3.9611e-01, 1.1598e-01, 4.9670e-02,  ..., 3.0953e-02,\n",
      "           5.4462e-02, 1.3406e-01],\n",
      "          [9.5649e-01, 5.5122e-03, 1.7178e-03,  ..., 7.7240e-04,\n",
      "           5.5225e-03, 2.1281e-02]],\n",
      "\n",
      "         [[8.7322e-01, 1.2766e-02, 6.7177e-03,  ..., 4.9143e-03,\n",
      "           1.8324e-02, 3.0757e-02],\n",
      "          [9.2550e-01, 3.9822e-02, 2.3739e-03,  ..., 3.2732e-05,\n",
      "           2.5018e-04, 2.5499e-02],\n",
      "          [6.1630e-01, 3.6286e-01, 8.9343e-04,  ..., 1.6912e-05,\n",
      "           1.7449e-04, 1.6116e-02],\n",
      "          ...,\n",
      "          [2.5042e-01, 1.9391e-06, 2.6813e-06,  ..., 9.4962e-04,\n",
      "           5.6487e-04, 4.1598e-02],\n",
      "          [4.9752e-02, 2.8136e-05, 5.5889e-07,  ..., 9.2636e-01,\n",
      "           7.8321e-04, 1.8919e-02],\n",
      "          [9.5245e-01, 3.2959e-03, 2.1282e-03,  ..., 3.0119e-03,\n",
      "           8.4601e-03, 1.5693e-02]],\n",
      "\n",
      "         [[4.5205e-01, 5.8991e-02, 3.3356e-02,  ..., 4.0197e-02,\n",
      "           9.3920e-02, 5.8892e-02],\n",
      "          [1.8236e-01, 6.8343e-02, 2.9233e-02,  ..., 2.1207e-02,\n",
      "           5.0253e-02, 1.8633e-01],\n",
      "          [3.1779e-01, 1.1254e-02, 1.4352e-01,  ..., 7.4084e-03,\n",
      "           2.6584e-02, 3.1842e-01],\n",
      "          ...,\n",
      "          [5.8816e-01, 1.3781e-02, 2.0832e-03,  ..., 1.8374e-01,\n",
      "           1.3742e-02, 1.7934e-01],\n",
      "          [5.0881e-01, 1.5886e-02, 3.4972e-03,  ..., 1.9945e-03,\n",
      "           3.8826e-01, 6.5822e-02],\n",
      "          [8.2853e-01, 1.0268e-02, 1.3342e-02,  ..., 1.1999e-02,\n",
      "           3.0431e-02, 5.2761e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3.9558e-01, 1.1355e-02, 1.1430e-02,  ..., 5.8708e-03,\n",
      "           1.4835e-02, 4.9157e-01],\n",
      "          [2.6543e-01, 7.2403e-02, 8.0691e-02,  ..., 4.4989e-03,\n",
      "           9.4270e-03, 4.7072e-01],\n",
      "          [4.0422e-01, 7.6856e-02, 6.9958e-03,  ..., 3.3038e-03,\n",
      "           4.2847e-03, 3.4661e-01],\n",
      "          ...,\n",
      "          [1.6128e-01, 1.3514e-02, 7.9941e-03,  ..., 1.4316e-03,\n",
      "           2.4376e-01, 2.7321e-01],\n",
      "          [1.9358e-01, 3.5729e-02, 5.3430e-02,  ..., 1.2657e-01,\n",
      "           1.6508e-02, 2.1200e-01],\n",
      "          [1.8357e-01, 1.8553e-03, 1.6040e-03,  ..., 2.0443e-03,\n",
      "           1.0630e-03, 8.0144e-01]],\n",
      "\n",
      "         [[3.3696e-01, 6.7461e-03, 9.6754e-03,  ..., 6.3543e-03,\n",
      "           1.6570e-02, 5.6920e-01],\n",
      "          [3.5903e-01, 6.0803e-02, 1.1000e-01,  ..., 1.3561e-02,\n",
      "           4.4395e-02, 1.6649e-01],\n",
      "          [3.3013e-01, 7.3194e-02, 4.0580e-02,  ..., 2.1237e-02,\n",
      "           4.6342e-02, 2.3515e-01],\n",
      "          ...,\n",
      "          [4.5094e-01, 3.7431e-03, 4.8818e-03,  ..., 4.4140e-03,\n",
      "           1.2794e-02, 2.8562e-01],\n",
      "          [2.6309e-01, 1.9644e-02, 2.1173e-02,  ..., 2.4789e-02,\n",
      "           1.2168e-02, 2.2930e-01],\n",
      "          [5.1890e-01, 4.8558e-04, 6.4863e-04,  ..., 5.2003e-04,\n",
      "           1.8039e-03, 4.6954e-01]],\n",
      "\n",
      "         [[3.3080e-01, 1.1844e-01, 7.6467e-02,  ..., 3.0573e-02,\n",
      "           8.1916e-02, 1.2924e-01],\n",
      "          [8.3236e-01, 7.1764e-02, 1.9284e-02,  ..., 3.2054e-04,\n",
      "           1.1940e-03, 6.4518e-02],\n",
      "          [6.1978e-01, 2.1387e-01, 4.6756e-02,  ..., 7.2137e-04,\n",
      "           1.8433e-03, 6.4722e-02],\n",
      "          ...,\n",
      "          [1.5030e-01, 2.2356e-02, 2.1026e-02,  ..., 2.7653e-02,\n",
      "           4.9643e-02, 1.8846e-01],\n",
      "          [9.7128e-02, 8.0257e-02, 3.2771e-02,  ..., 5.9938e-02,\n",
      "           3.6636e-02, 4.7657e-02],\n",
      "          [4.6308e-01, 4.0604e-03, 5.6631e-03,  ..., 1.9111e-03,\n",
      "           7.4570e-03, 4.9516e-01]]]]), tensor([[[[2.3523e-02, 6.0359e-03, 2.3946e-03,  ..., 4.2761e-04,\n",
      "           2.9145e-03, 9.5581e-01],\n",
      "          [6.0307e-02, 4.4250e-03, 9.7631e-05,  ..., 5.2434e-04,\n",
      "           4.3545e-04, 9.3031e-01],\n",
      "          [6.3658e-02, 6.0800e-04, 7.5361e-02,  ..., 9.8197e-05,\n",
      "           4.5308e-04, 8.5824e-01],\n",
      "          ...,\n",
      "          [4.9276e-02, 5.6218e-04, 1.7594e-04,  ..., 4.5145e-02,\n",
      "           3.4069e-05, 9.0374e-01],\n",
      "          [8.8812e-02, 1.2691e-03, 7.0237e-04,  ..., 1.0927e-05,\n",
      "           5.3319e-02, 8.5334e-01],\n",
      "          [9.0337e-03, 1.4014e-03, 7.5298e-04,  ..., 5.9677e-04,\n",
      "           3.1858e-04, 9.8202e-01]],\n",
      "\n",
      "         [[3.2691e-02, 4.9397e-02, 1.1624e-01,  ..., 5.4335e-02,\n",
      "           6.0532e-02, 1.0112e-01],\n",
      "          [8.6123e-02, 4.0047e-02, 8.9797e-02,  ..., 1.9429e-02,\n",
      "           1.8794e-02, 5.0365e-01],\n",
      "          [2.6431e-02, 2.5723e-02, 3.4144e-02,  ..., 8.7451e-03,\n",
      "           1.6785e-02, 7.5742e-01],\n",
      "          ...,\n",
      "          [4.1499e-02, 5.9249e-02, 4.6500e-02,  ..., 5.3578e-03,\n",
      "           2.0401e-02, 6.0133e-01],\n",
      "          [1.4678e-02, 1.7869e-01, 2.7977e-02,  ..., 2.6352e-02,\n",
      "           3.5040e-02, 2.0137e-01],\n",
      "          [8.2670e-04, 9.8515e-04, 1.5009e-03,  ..., 3.1959e-04,\n",
      "           2.3109e-04, 9.9375e-01]],\n",
      "\n",
      "         [[8.7630e-03, 5.5492e-03, 4.5502e-03,  ..., 2.6452e-03,\n",
      "           5.3559e-03, 9.4659e-01],\n",
      "          [2.7665e-01, 2.0597e-02, 1.7630e-03,  ..., 4.0162e-05,\n",
      "           1.1268e-03, 6.9841e-01],\n",
      "          [1.5041e-01, 4.2086e-01, 4.1090e-02,  ..., 3.3259e-05,\n",
      "           1.7762e-03, 3.7952e-01],\n",
      "          ...,\n",
      "          [4.2436e-03, 2.7109e-04, 7.7772e-04,  ..., 2.1653e-02,\n",
      "           1.2847e-02, 8.3719e-02],\n",
      "          [1.1229e-02, 2.3744e-03, 9.8752e-04,  ..., 5.6691e-02,\n",
      "           1.5053e-02, 9.7537e-02],\n",
      "          [1.6280e-02, 6.6712e-03, 7.8053e-03,  ..., 6.2369e-03,\n",
      "           7.6214e-03, 9.1050e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.6858e-02, 1.9572e-01, 2.4104e-02,  ..., 2.3350e-02,\n",
      "           8.0755e-02, 3.7396e-01],\n",
      "          [4.0770e-02, 1.5260e-01, 5.0985e-02,  ..., 2.0054e-03,\n",
      "           1.8731e-02, 6.1420e-01],\n",
      "          [8.1821e-02, 6.9643e-02, 4.6861e-03,  ..., 1.2932e-02,\n",
      "           6.1784e-02, 5.9674e-01],\n",
      "          ...,\n",
      "          [4.3694e-03, 5.4124e-04, 2.8636e-03,  ..., 1.3815e-03,\n",
      "           1.2644e-02, 7.2603e-02],\n",
      "          [1.6175e-02, 2.4743e-03, 9.7145e-03,  ..., 8.7992e-01,\n",
      "           3.9308e-03, 6.7828e-02],\n",
      "          [1.0468e-03, 6.2017e-03, 9.2214e-04,  ..., 3.7583e-04,\n",
      "           1.4444e-03, 9.8192e-01]],\n",
      "\n",
      "         [[1.0641e-01, 2.1028e-01, 1.9893e-01,  ..., 2.0808e-02,\n",
      "           1.0407e-01, 1.8222e-01],\n",
      "          [9.1900e-02, 1.0154e-01, 7.2842e-02,  ..., 1.6913e-02,\n",
      "           7.6243e-02, 5.2248e-01],\n",
      "          [1.8203e-01, 1.2564e-01, 6.7603e-02,  ..., 5.8154e-02,\n",
      "           4.8188e-02, 3.4481e-01],\n",
      "          ...,\n",
      "          [3.8466e-02, 7.4183e-02, 1.2071e-01,  ..., 1.8407e-02,\n",
      "           9.2853e-02, 2.4790e-01],\n",
      "          [3.7624e-02, 2.7920e-01, 1.7991e-01,  ..., 1.3413e-02,\n",
      "           7.2222e-02, 1.0358e-01],\n",
      "          [1.9825e-03, 3.0237e-04, 2.8984e-05,  ..., 2.5753e-04,\n",
      "           3.8547e-04, 9.9539e-01]],\n",
      "\n",
      "         [[4.7811e-02, 4.2272e-02, 2.5472e-02,  ..., 7.1403e-03,\n",
      "           6.3756e-02, 7.0715e-01],\n",
      "          [2.0215e-01, 3.7831e-02, 4.4225e-02,  ..., 1.3181e-02,\n",
      "           4.0221e-02, 5.2856e-01],\n",
      "          [1.9554e-01, 3.3188e-02, 1.7064e-03,  ..., 8.8776e-03,\n",
      "           1.7972e-02, 6.7032e-01],\n",
      "          ...,\n",
      "          [6.2747e-02, 3.7958e-02, 1.7061e-02,  ..., 2.5016e-03,\n",
      "           2.5080e-02, 7.8825e-01],\n",
      "          [1.2440e-01, 5.5386e-02, 3.1500e-02,  ..., 8.9676e-03,\n",
      "           2.0624e-02, 6.6127e-01],\n",
      "          [3.1266e-03, 1.9425e-03, 5.9707e-03,  ..., 1.5409e-03,\n",
      "           2.1822e-03, 9.7551e-01]]]]), tensor([[[[1.4496e-02, 7.9321e-03, 5.4048e-04,  ..., 1.4770e-03,\n",
      "           6.7743e-03, 9.6011e-01],\n",
      "          [2.9007e-01, 1.3697e-02, 2.5508e-03,  ..., 1.6637e-05,\n",
      "           2.3494e-03, 6.9079e-01],\n",
      "          [2.0065e-02, 8.6832e-01, 1.0137e-02,  ..., 1.4461e-05,\n",
      "           1.0631e-03, 9.3620e-02],\n",
      "          ...,\n",
      "          [3.2749e-04, 4.0719e-06, 5.8580e-05,  ..., 7.4007e-03,\n",
      "           1.9410e-03, 3.5026e-02],\n",
      "          [2.8380e-03, 9.4726e-04, 9.7927e-04,  ..., 9.0581e-01,\n",
      "           1.2377e-02, 5.9303e-02],\n",
      "          [6.7552e-03, 2.1408e-03, 5.7861e-04,  ..., 3.0690e-03,\n",
      "           2.6503e-03, 9.6994e-01]],\n",
      "\n",
      "         [[3.7645e-02, 3.2292e-03, 3.1600e-03,  ..., 2.0500e-03,\n",
      "           5.1360e-02, 8.8791e-01],\n",
      "          [5.3648e-02, 3.6042e-02, 1.3998e-03,  ..., 3.0066e-04,\n",
      "           1.0363e-02, 8.9764e-01],\n",
      "          [6.6907e-02, 3.5998e-01, 1.4627e-02,  ..., 4.6570e-04,\n",
      "           7.8402e-03, 5.4151e-01],\n",
      "          ...,\n",
      "          [6.6560e-03, 5.3296e-05, 6.5264e-04,  ..., 3.0450e-02,\n",
      "           6.4260e-03, 1.3139e-01],\n",
      "          [2.1667e-02, 9.7056e-04, 8.5867e-03,  ..., 4.8789e-01,\n",
      "           1.0858e-01, 2.6056e-01],\n",
      "          [1.3368e-02, 3.3879e-03, 9.1712e-04,  ..., 4.4383e-03,\n",
      "           1.6976e-02, 9.4297e-01]],\n",
      "\n",
      "         [[4.3916e-02, 1.6185e-02, 2.2832e-02,  ..., 7.0136e-04,\n",
      "           5.3594e-03, 8.3402e-01],\n",
      "          [4.4031e-02, 5.5322e-02, 2.4671e-02,  ..., 1.6089e-05,\n",
      "           4.6536e-04, 8.3933e-01],\n",
      "          [3.3827e-02, 3.0055e-02, 7.4518e-03,  ..., 1.0471e-03,\n",
      "           1.7336e-02, 7.0230e-01],\n",
      "          ...,\n",
      "          [1.6275e-02, 1.1146e-03, 7.1607e-04,  ..., 2.5451e-02,\n",
      "           1.2557e-01, 7.4090e-01],\n",
      "          [3.2564e-02, 5.2516e-03, 6.3631e-03,  ..., 3.5522e-03,\n",
      "           3.4813e-03, 8.3791e-01],\n",
      "          [8.4209e-03, 4.0047e-03, 2.0957e-03,  ..., 1.8745e-03,\n",
      "           7.6677e-04, 9.7210e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3.5223e-03, 5.6400e-03, 2.7136e-03,  ..., 6.7574e-04,\n",
      "           7.5617e-04, 9.8447e-01],\n",
      "          [1.6192e-02, 1.9429e-02, 2.3184e-03,  ..., 1.2763e-04,\n",
      "           9.9198e-04, 9.5927e-01],\n",
      "          [4.1866e-03, 1.8924e-01, 2.2878e-02,  ..., 6.1323e-05,\n",
      "           2.5991e-04, 7.8167e-01],\n",
      "          ...,\n",
      "          [1.5858e-03, 1.7700e-05, 5.9130e-05,  ..., 1.1342e-02,\n",
      "           4.0320e-04, 9.0589e-01],\n",
      "          [8.9626e-03, 5.3456e-04, 1.3615e-03,  ..., 1.8342e-02,\n",
      "           1.9060e-03, 9.5733e-01],\n",
      "          [1.3695e-03, 2.4291e-03, 1.5470e-03,  ..., 1.1064e-03,\n",
      "           8.9509e-04, 9.8988e-01]],\n",
      "\n",
      "         [[4.0179e-02, 5.5832e-02, 7.0998e-02,  ..., 4.8184e-02,\n",
      "           2.9687e-02, 5.2857e-01],\n",
      "          [1.0646e-02, 1.2826e-02, 7.1015e-03,  ..., 3.7784e-04,\n",
      "           1.9126e-03, 9.6063e-01],\n",
      "          [2.2632e-02, 1.0720e-01, 4.4540e-02,  ..., 4.2906e-04,\n",
      "           2.3141e-03, 8.0721e-01],\n",
      "          ...,\n",
      "          [5.2394e-03, 3.8369e-03, 2.4044e-02,  ..., 1.7366e-02,\n",
      "           5.7945e-03, 4.9022e-01],\n",
      "          [1.0756e-02, 2.3233e-02, 7.0557e-02,  ..., 2.7359e-02,\n",
      "           1.7355e-02, 2.4333e-01],\n",
      "          [2.9914e-03, 3.8791e-03, 6.7346e-03,  ..., 8.8978e-04,\n",
      "           7.8783e-04, 9.7461e-01]],\n",
      "\n",
      "         [[9.1843e-03, 3.0034e-02, 8.0162e-03,  ..., 1.3415e-02,\n",
      "           6.8307e-03, 8.8124e-01],\n",
      "          [4.6855e-03, 1.0871e-02, 1.7675e-03,  ..., 5.8092e-04,\n",
      "           1.0860e-03, 9.7324e-01],\n",
      "          [3.4190e-02, 1.5514e-01, 1.3473e-02,  ..., 1.1669e-04,\n",
      "           5.4434e-03, 7.8284e-01],\n",
      "          ...,\n",
      "          [1.4808e-02, 5.3203e-04, 9.0917e-03,  ..., 3.0991e-02,\n",
      "           1.3228e-02, 6.5304e-01],\n",
      "          [3.0162e-02, 5.4189e-03, 1.0785e-02,  ..., 1.2035e-02,\n",
      "           6.2513e-03, 8.4165e-01],\n",
      "          [1.0952e-03, 2.5775e-03, 9.4823e-04,  ..., 3.6588e-04,\n",
      "           5.9134e-04, 9.8683e-01]]]]), tensor([[[[1.0154e-02, 1.2995e-02, 5.2029e-02,  ..., 2.5263e-03,\n",
      "           3.7192e-02, 8.4502e-01],\n",
      "          [5.0348e-02, 3.9012e-02, 2.4534e-01,  ..., 3.2130e-03,\n",
      "           3.2432e-02, 5.2729e-01],\n",
      "          [5.3210e-02, 1.8836e-02, 4.9306e-02,  ..., 1.7054e-02,\n",
      "           8.3335e-02, 5.8770e-01],\n",
      "          ...,\n",
      "          [7.1527e-03, 1.4556e-03, 1.6223e-03,  ..., 1.7494e-02,\n",
      "           3.1295e-02, 9.1054e-01],\n",
      "          [9.6338e-03, 4.4123e-03, 1.4487e-02,  ..., 9.3857e-03,\n",
      "           4.1303e-02, 8.7225e-01],\n",
      "          [6.9632e-03, 1.7151e-03, 1.5557e-03,  ..., 1.1886e-03,\n",
      "           2.9719e-03, 9.7824e-01]],\n",
      "\n",
      "         [[1.7163e-02, 4.7796e-02, 1.1037e-01,  ..., 5.0518e-03,\n",
      "           1.9606e-02, 6.8631e-01],\n",
      "          [6.3648e-03, 5.0155e-01, 1.6079e-02,  ..., 4.8838e-04,\n",
      "           1.9950e-03, 3.9021e-01],\n",
      "          [1.3642e-02, 3.2136e-02, 6.0891e-01,  ..., 7.6364e-04,\n",
      "           1.7404e-03, 3.2317e-01],\n",
      "          ...,\n",
      "          [2.4666e-03, 7.1553e-04, 3.7406e-03,  ..., 5.4429e-01,\n",
      "           8.8859e-03, 1.0101e-01],\n",
      "          [2.8543e-02, 6.7468e-03, 1.1048e-02,  ..., 1.8260e-01,\n",
      "           3.5093e-01, 2.7810e-01],\n",
      "          [8.6297e-04, 1.3443e-03, 1.6465e-03,  ..., 7.0665e-04,\n",
      "           1.1043e-03, 9.9072e-01]],\n",
      "\n",
      "         [[4.0697e-02, 6.3652e-02, 3.6576e-02,  ..., 7.7367e-03,\n",
      "           1.0354e-02, 7.3083e-01],\n",
      "          [2.9472e-02, 4.1275e-02, 1.3600e-02,  ..., 2.8593e-03,\n",
      "           3.6857e-03, 8.5627e-01],\n",
      "          [2.4214e-02, 9.2748e-02, 8.8026e-03,  ..., 4.7044e-03,\n",
      "           5.5708e-03, 7.3907e-01],\n",
      "          ...,\n",
      "          [9.3758e-03, 1.5152e-03, 7.5101e-03,  ..., 3.6728e-02,\n",
      "           6.7907e-03, 6.1533e-01],\n",
      "          [1.2385e-02, 5.2349e-03, 2.0349e-02,  ..., 2.7077e-01,\n",
      "           7.7041e-03, 6.0024e-01],\n",
      "          [3.3822e-04, 5.8917e-04, 2.3978e-04,  ..., 3.2190e-04,\n",
      "           3.5940e-04, 9.9530e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[9.5590e-02, 1.3794e-01, 1.0436e-01,  ..., 3.0369e-02,\n",
      "           3.3079e-02, 4.0863e-01],\n",
      "          [1.6467e-02, 5.4305e-02, 2.2339e-01,  ..., 1.4941e-02,\n",
      "           1.6721e-02, 4.2382e-01],\n",
      "          [1.6277e-02, 2.7758e-02, 7.0417e-02,  ..., 3.8901e-02,\n",
      "           4.6678e-02, 3.0581e-01],\n",
      "          ...,\n",
      "          [1.3781e-03, 2.8918e-03, 9.1693e-03,  ..., 1.6912e-02,\n",
      "           3.5500e-03, 9.1855e-01],\n",
      "          [4.9126e-03, 3.2498e-02, 3.9904e-02,  ..., 3.6991e-02,\n",
      "           1.3068e-02, 7.7474e-01],\n",
      "          [2.3644e-04, 1.1518e-03, 3.3632e-04,  ..., 1.6759e-04,\n",
      "           1.8178e-04, 9.9651e-01]],\n",
      "\n",
      "         [[7.8468e-03, 2.5893e-02, 5.6988e-02,  ..., 1.2501e-02,\n",
      "           2.7216e-02, 7.5298e-01],\n",
      "          [1.0665e-02, 4.4820e-02, 9.5816e-02,  ..., 1.4804e-02,\n",
      "           2.7879e-02, 6.7797e-01],\n",
      "          [4.7052e-02, 6.2929e-02, 9.6478e-02,  ..., 1.6041e-02,\n",
      "           6.7584e-02, 5.7181e-01],\n",
      "          ...,\n",
      "          [6.2406e-03, 1.5899e-02, 8.2431e-03,  ..., 7.4044e-03,\n",
      "           1.9630e-02, 8.6786e-01],\n",
      "          [2.5566e-02, 6.9945e-02, 5.5487e-02,  ..., 1.3150e-02,\n",
      "           6.2667e-02, 6.2550e-01],\n",
      "          [9.9505e-04, 1.0042e-03, 1.0440e-03,  ..., 9.4922e-04,\n",
      "           5.4472e-04, 9.8969e-01]],\n",
      "\n",
      "         [[6.5113e-03, 6.7234e-03, 2.0530e-02,  ..., 1.1797e-03,\n",
      "           1.1946e-02, 9.4318e-01],\n",
      "          [7.8791e-03, 9.8340e-03, 2.8098e-01,  ..., 1.3805e-04,\n",
      "           6.7948e-03, 6.7354e-01],\n",
      "          [6.5481e-03, 5.3857e-03, 2.1779e-02,  ..., 1.0056e-02,\n",
      "           3.8935e-02, 3.8434e-01],\n",
      "          ...,\n",
      "          [4.9350e-03, 3.1281e-03, 2.3158e-03,  ..., 6.3418e-02,\n",
      "           2.5210e-01, 6.3851e-01],\n",
      "          [6.2544e-03, 2.0361e-02, 7.2745e-03,  ..., 3.2215e-03,\n",
      "           6.0261e-03, 9.4807e-01],\n",
      "          [2.0387e-03, 1.0582e-03, 5.2607e-04,  ..., 2.1763e-03,\n",
      "           2.1480e-03, 9.8621e-01]]]]), tensor([[[[1.1406e-02, 5.4423e-02, 4.2400e-02,  ..., 1.3282e-02,\n",
      "           1.2351e-01, 6.3486e-01],\n",
      "          [7.3369e-03, 2.4761e-02, 2.1826e-02,  ..., 9.9122e-03,\n",
      "           2.0029e-02, 8.3019e-01],\n",
      "          [6.5776e-03, 2.0242e-02, 3.0630e-02,  ..., 1.1345e-02,\n",
      "           2.1038e-02, 8.1124e-01],\n",
      "          ...,\n",
      "          [1.7475e-03, 6.6373e-03, 4.1268e-03,  ..., 2.9172e-03,\n",
      "           4.9918e-03, 9.4579e-01],\n",
      "          [4.4245e-03, 1.2658e-02, 9.4178e-03,  ..., 8.7999e-03,\n",
      "           7.9426e-03, 9.0757e-01],\n",
      "          [2.5142e-02, 1.4450e-01, 1.1329e-01,  ..., 7.1446e-02,\n",
      "           1.1102e-01, 8.1050e-02]],\n",
      "\n",
      "         [[8.4279e-03, 8.8439e-02, 1.4346e-01,  ..., 6.2257e-03,\n",
      "           3.1439e-02, 5.7450e-01],\n",
      "          [3.9265e-03, 8.0598e-02, 4.0033e-02,  ..., 2.3673e-03,\n",
      "           6.7836e-03, 7.8934e-01],\n",
      "          [9.2046e-03, 3.8518e-02, 1.9456e-01,  ..., 7.3445e-03,\n",
      "           1.8135e-02, 6.5727e-01],\n",
      "          ...,\n",
      "          [5.0093e-03, 7.3590e-03, 1.8850e-02,  ..., 4.2821e-02,\n",
      "           1.0869e-02, 8.2974e-01],\n",
      "          [5.7371e-03, 2.4105e-02, 2.3453e-02,  ..., 9.2756e-03,\n",
      "           8.3638e-03, 8.5529e-01],\n",
      "          [1.7107e-02, 1.0342e-01, 9.3716e-02,  ..., 3.7425e-02,\n",
      "           6.2374e-02, 4.2587e-01]],\n",
      "\n",
      "         [[8.4971e-03, 3.1907e-02, 1.9772e-02,  ..., 7.0038e-03,\n",
      "           2.3465e-02, 8.0884e-01],\n",
      "          [2.0511e-02, 4.1137e-01, 8.4619e-03,  ..., 1.4091e-03,\n",
      "           7.2062e-03, 3.9154e-01],\n",
      "          [3.4518e-02, 9.3381e-03, 7.8284e-01,  ..., 5.7173e-03,\n",
      "           1.2245e-02, 1.1459e-01],\n",
      "          ...,\n",
      "          [3.5148e-02, 5.4726e-03, 9.2967e-03,  ..., 8.3032e-01,\n",
      "           8.2386e-03, 7.9553e-02],\n",
      "          [6.1420e-02, 2.2145e-02, 2.5771e-02,  ..., 2.5518e-02,\n",
      "           5.5724e-01, 1.1723e-01],\n",
      "          [7.2243e-02, 1.2741e-01, 8.4313e-02,  ..., 5.4058e-02,\n",
      "           5.4908e-02, 1.8097e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3.3978e-02, 1.6410e-01, 1.1552e-01,  ..., 1.2504e-02,\n",
      "           8.8236e-02, 3.9140e-01],\n",
      "          [5.6064e-02, 6.9979e-02, 2.2749e-02,  ..., 5.1973e-03,\n",
      "           7.4947e-03, 8.0221e-01],\n",
      "          [1.7099e-02, 9.2548e-02, 5.1126e-02,  ..., 2.3789e-02,\n",
      "           1.6781e-02, 6.5584e-01],\n",
      "          ...,\n",
      "          [1.3912e-02, 1.4052e-02, 3.8514e-02,  ..., 1.0286e-01,\n",
      "           2.0229e-02, 5.6036e-01],\n",
      "          [4.0130e-02, 3.5452e-02, 1.4105e-01,  ..., 1.9502e-01,\n",
      "           1.3853e-02, 2.7355e-01],\n",
      "          [8.8804e-02, 8.8671e-02, 1.2403e-01,  ..., 7.5833e-02,\n",
      "           8.5094e-02, 1.8650e-01]],\n",
      "\n",
      "         [[5.9744e-03, 6.3481e-02, 5.9452e-02,  ..., 6.2955e-03,\n",
      "           1.1549e-02, 7.5085e-01],\n",
      "          [1.2402e-02, 1.2657e-01, 2.7113e-02,  ..., 4.5522e-03,\n",
      "           9.5324e-03, 7.2138e-01],\n",
      "          [1.1520e-02, 3.6844e-02, 1.9397e-01,  ..., 2.1498e-02,\n",
      "           1.5725e-02, 6.0318e-01],\n",
      "          ...,\n",
      "          [9.0691e-04, 3.1453e-03, 5.1255e-03,  ..., 3.6618e-02,\n",
      "           3.8982e-03, 8.1529e-01],\n",
      "          [1.1336e-03, 1.6276e-02, 1.6874e-02,  ..., 3.8945e-02,\n",
      "           7.4941e-03, 8.4849e-01],\n",
      "          [1.5038e-02, 1.0446e-01, 9.8268e-02,  ..., 7.1941e-02,\n",
      "           8.5406e-02, 3.0406e-01]],\n",
      "\n",
      "         [[3.2988e-02, 6.0303e-02, 1.2768e-01,  ..., 4.0737e-02,\n",
      "           1.8617e-01, 4.3247e-01],\n",
      "          [8.1455e-03, 6.1155e-01, 2.0962e-02,  ..., 1.8719e-03,\n",
      "           3.3953e-03, 2.6569e-01],\n",
      "          [8.8109e-03, 3.4678e-03, 8.4852e-01,  ..., 1.0964e-02,\n",
      "           3.3151e-03, 1.0835e-01],\n",
      "          ...,\n",
      "          [7.1405e-03, 4.1228e-04, 1.4162e-03,  ..., 9.3449e-01,\n",
      "           1.9506e-03, 3.6500e-02],\n",
      "          [4.2705e-02, 8.5839e-03, 1.0245e-02,  ..., 9.5076e-02,\n",
      "           6.5418e-01, 1.1402e-01],\n",
      "          [7.4343e-02, 1.2466e-01, 8.8369e-02,  ..., 7.3755e-02,\n",
      "           1.2414e-01, 1.2124e-01]]]])))\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-cased\", output_attentions=True, output_hidden_states=True)\n",
    "model.eval()\n",
    "\n",
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    model_output = model(**model_inputs)\n",
    "\n",
    "\n",
    "print(\"Hidden state size (per layer):  \", model_output.hidden_states[0].shape)\n",
    "print(\"Attention head size (per layer):\", model_output.attentions[0].shape)     # (layer, batch, query_word_idx, key_word_idxs)\n",
    "                                                                               # y-axis is query, x-axis is key\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Function to create one-hot encoding for authors\n",
    "# Function to create integer label for authors\n",
    "def encode_author(author, authors_list):\n",
    "    return authors_list.index(author)\n",
    "\n",
    "# Function to split text into chunks of equal length\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Define authors and paths\n",
    "authors = os.listdir(\"book_datasets\")\n",
    "data = {'text': [], 'label': []}\n",
    "\n",
    "# Iterate over authors and text files\n",
    "for author in authors:\n",
    "    author_path = os.path.join(\"book_datasets\", author)\n",
    "    author_files = [f for f in os.listdir(author_path) if f.endswith(\".txt\")]\n",
    "\n",
    "    for file in author_files:\n",
    "        file_path = os.path.join(author_path, file)\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            \n",
    "            # Split text into chunks of 512 tokens\n",
    "            chunks = split_into_chunks(text, 512)\n",
    "            \n",
    "            # Create one-hot encoding for each chunk\n",
    "            for chunk in chunks:\n",
    "                label = encode_author(author, authors)\n",
    "                data['text'].append(chunk)\n",
    "                data['label'].append(label)\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(data['text'])\n",
    "random.shuffle(data['label'])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(data['text']))\n",
    "train_data = {'text': data['text'][:train_size], 'label': data['label'][:train_size]}\n",
    "val_data = {'text': data['text'][train_size:], 'label': data['label'][train_size:]}\n",
    "\n",
    "# Create datasets\n",
    "book_train_dataset = Dataset.from_dict(train_data)\n",
    "book_val_dataset = Dataset.from_dict(val_data)\n",
    "\n",
    "# Create a DatasetDict\n",
    "book_dataset = DatasetDict({'train': book_train_dataset, 'val': book_val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5135\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1284\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 5135/5135 [00:00<00:00, 10128.09 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 1284/1284 [00:00<00:00, 9173.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "tokenized_dataset = book_dataset.map(lambda example: tokenizer(example['text'], truncation=True), batched=True)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([0, 0]),\n",
       " 'input_ids': [tensor([  101,  1142,  1721,  1107,  1240,  1493,   117,  1240,  2252,   787,\n",
       "            188,  1567,  1110,  1107,  1240,  1493,   119,  1790,   787,   189,\n",
       "          12477,  1197,  1241,  1111,  1140,   119,   164,   168, 13832,  2083,\n",
       "            168,   156, 18172,   155,  2346, 27211, 10460, 24890, 17656, 12880,\n",
       "           2069,  2249,   119,   166,   156, 18172,   155,  2346, 27211, 10460,\n",
       "          24890, 17656, 12880,  2069,  2249,   119, 20286,   117,  1303,  1110,\n",
       "           1103,  5039,  1104,  1139,  2998,   119, 17604,   146,  2373,  1122,\n",
       "           1106,  1128,   136, 10722,  2137,  3663, 24890, 17656, 12880,  2069,\n",
       "           2249,   119,  2421,  1143,  1267,  1122,   119,   164,   156, 18172,\n",
       "            155,  2346, 27211, 10460,   168,  1493,  1123,  1103,  2998,   168,\n",
       "            119,   168,  1153,  9568,  1122,   168,   117,   168,  1105,  1173,\n",
       "            168,   117,   168,  1114,   170,  8982,  1104,  7615,   168,   117,\n",
       "            168,  3632,  1122,  1146,   168,   119,   166,   156, 18172,   155,\n",
       "           2346, 27211, 10460, 24890, 17656, 12880,  2069,  2249,   119,  1327,\n",
       "           1132,  1128,  1833,   136, 10722,  2137,  3663, 24890, 17656, 12880,\n",
       "           2069,  2249,   119,   138,  1299,   787,   188,  1297,  1110,  1104,\n",
       "           1167,  2860,  1190,   170,  1590,   787,   188,   119,  1135,  1144,\n",
       "           2610,  2492,   117,  6815,  9668,   117,   102]),\n",
       "  tensor([  101,   183,   170, 10335,  4873,  7180,  1105, 12543,  1111,  1143,\n",
       "            119,   164,   168, 19588,   119,   168,   166,   143, 18172,  9272,\n",
       "            160, 12150, 23258,   119,  3435,   117,  1519,   787,   188,  1294,\n",
       "           1144,  1566,   132,  1131,   787,  1325,  1770,  1129,  1171,  1254,\n",
       "            119,   164,   168, 16409, 14272,  2227,   119,   168,   166,  9314,\n",
       "          11680,  2036,  7118,   119,  1370,  4894,   119,   138,  7043,  1107,\n",
       "           1103,  5130,   119, 13832,  2083, 23157,  1105,  1330,  2188,   119,\n",
       "            149, 11680,  2249, 24569,   119,  1422,  1393, 14948,  1138,  1133,\n",
       "           1855,  1240,  3578,   117,  5979,  1169, 19348,  8791,   131,  1178,\n",
       "            117,   146,  1474,   117, 11675,   787,   188,  1138,  1151, 16352,\n",
       "          16908,   119,  1109,   176, 19366,  4179,  6809,  3982,  7172,  4830,\n",
       "           1104,  6603, 16100,   131,   783,  5390,   117,  1119,  1108,  2044,\n",
       "            131,   783,  1262,  1103,  1268,   191, 19457,  2227, 18393,  4426,\n",
       "          11848,  2647,   787,   173,  1315,  1523,   132,  2627,  1306,   117,\n",
       "           1128,  1336,  1474,   117,  1191,   787,   189,  4268,  1128,   117,\n",
       "            143, 21649,  2093,  2311,   787,   173,   117,   102])],\n",
       " 'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify GPU device for Trainer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=3)\n",
    "\n",
    "arguments = TrainingArguments(\n",
    "    output_dir=\"sample_hf_trainer\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=224\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # calculates the accuracy\n",
    "    return {\"accuracy\": np.mean(predictions == labels)}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=arguments,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, EarlyStoppingCallback\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(json.dumps(logs) + \"\\n\")\n",
    "\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.0))\n",
    "trainer.add_callback(LoggingCallback(\"sample_hf_trainer/log.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/963 [52:10<?, ?it/s]\n",
      "  1%|          | 10/963 [49:57<79:21:11, 299.76s/it]\n",
      "  0%|          | 1/963 [00:07<2:02:38,  7.65s/it]"
     ]
    }
   ],
   "source": [
    "# Train for a few steps\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'label'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1109,  4042,  ...,  1114,  1103,   102],\n",
       "         [  101,  1109,  4042,  ...,   168,  5706,   102],\n",
       "         [  101,  1109,  4042,  ..., 25370, 16972,   102],\n",
       "         ...,\n",
       "         [  101,  1109,  4042,  ...,   117,   790,   102],\n",
       "         [  101,  1109,  4042,  ...,  1760, 15380,   102],\n",
       "         [  101,  1109,  4042,  ...,  7462,  1787,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': tensor([[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]], dtype=torch.float64)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\ProgrammingWindows\\Anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/3 [09:11<?, ?it/s]\n",
      "  0%|          | 0/3 [06:41<?, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\WIndowsRepositories\\NLP\\Tarea_6\\Punto_1_2.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WIndowsRepositories/NLP/Tarea_6/Punto_1_2.ipynb#X13sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m trainer\u001b[39m.\u001b[39madd_callback(EarlyStoppingCallback(early_stopping_patience\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, early_stopping_threshold\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WIndowsRepositories/NLP/Tarea_6/Punto_1_2.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/WIndowsRepositories/NLP/Tarea_6/Punto_1_2.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32md:\\ProgrammingWindows\\Anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32md:\\ProgrammingWindows\\Anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\transformers\\trainer.py:1838\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1835\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1837\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1838\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1839\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1840\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32md:\\ProgrammingWindows\\Anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\accelerate\\data_loader.py:451\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    452\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgrammingWindows\\Anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\ProgrammingWindows\\Anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\ProgrammingWindows\\Anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\ProgrammingWindows\\Anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"fine-tuned_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=224\n",
    ")\n",
    "\n",
    "# Custom compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    accuracy = np.mean(preds == labels)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Model, tokenizer, and trainer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Add early stopping callback\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.0))\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
