{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "group_code = \"santiagomartinez_201533279_camilocastaneda_202314092\"\n",
    "segment_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def processing_text(texto):\n",
    "    # Paso 1: Remover con un expresión regular carateres especiales (no palabras).\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(texto))\n",
    "    # Paso 2: Remover ocurrencias de caracteres individuales\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "    # Paso 3: Remover números (Ocurrencias muy esporádicas en nuestro dataset)\n",
    "    processed_feature = re.sub(r'[0-9]+', ' ', processed_feature)\n",
    "    # Paso 4: Simplificar espacios concecutivos a un único espacio entre palabras\n",
    "    processed_feature = re.sub(' +', ' ', processed_feature)\n",
    "    # Paso 5: Pasar todo el texto a minúsculas\n",
    "    processed_feature = processed_feature.lower()\n",
    "\n",
    "    return processed_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the text data\n",
    "def load_and_preprocess_text_data(folder_path, segment_length):\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    for author in authors:\n",
    "        author_folder = os.path.join(folder_path, author)\n",
    "        for filename in os.listdir(author_folder):\n",
    "            with open(os.path.join(author_folder, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "                # Process the text\n",
    "                processed_text = processing_text(text)\n",
    "                # Split text into segments if needed\n",
    "                sequences = [processed_text[i:i+segment_length] for i in range(0, len(processed_text), segment_length)]\n",
    "                texts.extend(sequences)\n",
    "                labels.extend([author] * len(sequences))\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "# Read and preprocess the text data\n",
    "def get_corpus(folder_path):\n",
    "    texts = []\n",
    "    invalid_txt = ['']\n",
    "    \n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_ in files:\n",
    "            with open(os.path.join(root, file_), \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    pre_processed_text = processing_text(line)\n",
    "                    if pre_processed_text not in invalid_txt:\n",
    "                        texts.append(pre_processed_text)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 20628\n"
     ]
    }
   ],
   "source": [
    "# Get authors\n",
    "authors = os.listdir(\"book_datasets\")\n",
    "\n",
    "# Initialize and fit the Keras tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "corpus = get_corpus(\"book_datasets\")\n",
    "corpus = [sentence for sentence in corpus if sentence!=' ' and sentence!='' and len(sentence.split())>3]\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"vocab size: \" + str(vocab_size))\n",
    "# Load and preprocess the text data\n",
    "texts, labels = load_and_preprocess_text_data(\"book_datasets\", segment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels and one-hot encode\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(authors)\n",
    "labels_encoded = label_encoder.transform(labels)\n",
    "labels_one_hot = to_categorical(labels_encoded, num_classes=len(authors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels_one_hot, test_size=0.2, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(test_texts, test_labels, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text data into numpy arrays\n",
    "train_texts = np.array(train_texts)\n",
    "train_labels = np.array(train_labels)\n",
    "val_texts = np.array(val_texts)\n",
    "val_labels = np.array(val_labels)\n",
    "test_texts = np.array(test_texts)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 500)               504687500 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               64128     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 504776591 (1.88 GB)\n",
      "Trainable params: 89091 (348.01 KB)\n",
      "Non-trainable params: 504687500 (1.88 GB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Word2Vec model from TensorFlow Hub\n",
    "embedding_model_url = \"https://tfhub.dev/google/Wiki-words-500/2\"\n",
    "hub_layer = hub.KerasLayer(embedding_model_url, input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "# Define your feed forward neural network\n",
    "model = tf.keras.Sequential([\n",
    "    hub_layer,  # The Word2Vec embedding layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(authors), activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "184/184 [==============================] - 2s 6ms/step - loss: 0.9371 - accuracy: 0.5474 - val_loss: 0.8291 - val_accuracy: 0.6372\n",
      "Epoch 2/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.8094 - accuracy: 0.6367 - val_loss: 0.7190 - val_accuracy: 0.6712\n",
      "Epoch 3/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.7630 - accuracy: 0.6504 - val_loss: 0.6746 - val_accuracy: 0.6855\n",
      "Epoch 4/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.7322 - accuracy: 0.6653 - val_loss: 0.6444 - val_accuracy: 0.7147\n",
      "Epoch 5/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.6977 - accuracy: 0.6917 - val_loss: 0.6157 - val_accuracy: 0.7249\n",
      "Epoch 6/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.6601 - accuracy: 0.7150 - val_loss: 0.5656 - val_accuracy: 0.7717\n",
      "Epoch 7/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.6329 - accuracy: 0.7231 - val_loss: 0.5091 - val_accuracy: 0.7901\n",
      "Epoch 8/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.6138 - accuracy: 0.7396 - val_loss: 0.5564 - val_accuracy: 0.7914\n",
      "Epoch 9/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5979 - accuracy: 0.7428 - val_loss: 0.5536 - val_accuracy: 0.7588\n",
      "Epoch 10/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5824 - accuracy: 0.7531 - val_loss: 0.4935 - val_accuracy: 0.7887\n",
      "Epoch 11/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5743 - accuracy: 0.7568 - val_loss: 0.5349 - val_accuracy: 0.7697\n",
      "Epoch 12/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5589 - accuracy: 0.7648 - val_loss: 0.4556 - val_accuracy: 0.8179\n",
      "Epoch 13/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5499 - accuracy: 0.7701 - val_loss: 0.4579 - val_accuracy: 0.8071\n",
      "Epoch 14/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5586 - accuracy: 0.7668 - val_loss: 0.4680 - val_accuracy: 0.8179\n",
      "Epoch 15/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5515 - accuracy: 0.7686 - val_loss: 0.4604 - val_accuracy: 0.8220\n",
      "Epoch 16/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5524 - accuracy: 0.7644 - val_loss: 0.4767 - val_accuracy: 0.8091\n",
      "Epoch 17/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5299 - accuracy: 0.7779 - val_loss: 0.4363 - val_accuracy: 0.8207\n",
      "Epoch 18/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5304 - accuracy: 0.7802 - val_loss: 0.4623 - val_accuracy: 0.8010\n",
      "Epoch 19/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5168 - accuracy: 0.7841 - val_loss: 0.4476 - val_accuracy: 0.8091\n",
      "Epoch 20/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5248 - accuracy: 0.7837 - val_loss: 0.4488 - val_accuracy: 0.8118\n",
      "Epoch 21/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5124 - accuracy: 0.7859 - val_loss: 0.4701 - val_accuracy: 0.7948\n",
      "Epoch 22/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5255 - accuracy: 0.7833 - val_loss: 0.4239 - val_accuracy: 0.8329\n",
      "Epoch 23/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5138 - accuracy: 0.7778 - val_loss: 0.4344 - val_accuracy: 0.8152\n",
      "Epoch 24/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5018 - accuracy: 0.7864 - val_loss: 0.4293 - val_accuracy: 0.8288\n",
      "Epoch 25/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5057 - accuracy: 0.7897 - val_loss: 0.4144 - val_accuracy: 0.8268\n",
      "Epoch 26/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4903 - accuracy: 0.7929 - val_loss: 0.4213 - val_accuracy: 0.8247\n",
      "Epoch 27/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4982 - accuracy: 0.7911 - val_loss: 0.4453 - val_accuracy: 0.8220\n",
      "Epoch 28/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4889 - accuracy: 0.7903 - val_loss: 0.4372 - val_accuracy: 0.8152\n",
      "Epoch 29/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4895 - accuracy: 0.7955 - val_loss: 0.4449 - val_accuracy: 0.8227\n",
      "Epoch 30/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4941 - accuracy: 0.7925 - val_loss: 0.4172 - val_accuracy: 0.8302\n",
      "Epoch 31/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5072 - accuracy: 0.7840 - val_loss: 0.4554 - val_accuracy: 0.7996\n",
      "Epoch 32/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.5040 - accuracy: 0.7788 - val_loss: 0.4241 - val_accuracy: 0.8363\n",
      "Epoch 33/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4912 - accuracy: 0.7934 - val_loss: 0.4163 - val_accuracy: 0.8315\n",
      "Epoch 34/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4846 - accuracy: 0.7929 - val_loss: 0.4126 - val_accuracy: 0.8322\n",
      "Epoch 35/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4927 - accuracy: 0.7963 - val_loss: 0.4267 - val_accuracy: 0.8207\n",
      "Epoch 36/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4769 - accuracy: 0.8014 - val_loss: 0.4113 - val_accuracy: 0.8397\n",
      "Epoch 37/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4939 - accuracy: 0.7913 - val_loss: 0.4425 - val_accuracy: 0.8105\n",
      "Epoch 38/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4813 - accuracy: 0.7946 - val_loss: 0.4259 - val_accuracy: 0.8268\n",
      "Epoch 39/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4848 - accuracy: 0.7948 - val_loss: 0.4185 - val_accuracy: 0.8295\n",
      "Epoch 40/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4813 - accuracy: 0.7957 - val_loss: 0.4182 - val_accuracy: 0.8329\n",
      "Epoch 41/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4846 - accuracy: 0.7961 - val_loss: 0.4084 - val_accuracy: 0.8302\n",
      "Epoch 42/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4817 - accuracy: 0.7966 - val_loss: 0.4029 - val_accuracy: 0.8410\n",
      "Epoch 43/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4678 - accuracy: 0.8043 - val_loss: 0.4205 - val_accuracy: 0.8308\n",
      "Epoch 44/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4649 - accuracy: 0.7983 - val_loss: 0.4320 - val_accuracy: 0.8254\n",
      "Epoch 45/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4620 - accuracy: 0.8055 - val_loss: 0.4093 - val_accuracy: 0.8274\n",
      "Epoch 46/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4654 - accuracy: 0.8060 - val_loss: 0.4147 - val_accuracy: 0.8254\n",
      "Epoch 47/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4595 - accuracy: 0.8110 - val_loss: 0.4160 - val_accuracy: 0.8227\n",
      "Epoch 48/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4447 - accuracy: 0.8125 - val_loss: 0.4030 - val_accuracy: 0.8315\n",
      "Epoch 49/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4599 - accuracy: 0.8068 - val_loss: 0.4099 - val_accuracy: 0.8342\n",
      "Epoch 50/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4435 - accuracy: 0.8175 - val_loss: 0.4016 - val_accuracy: 0.8322\n",
      "Epoch 51/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4457 - accuracy: 0.8099 - val_loss: 0.4055 - val_accuracy: 0.8390\n",
      "Epoch 52/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4501 - accuracy: 0.8099 - val_loss: 0.4096 - val_accuracy: 0.8234\n",
      "Epoch 53/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4461 - accuracy: 0.8125 - val_loss: 0.4054 - val_accuracy: 0.8336\n",
      "Epoch 54/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4449 - accuracy: 0.8107 - val_loss: 0.3989 - val_accuracy: 0.8404\n",
      "Epoch 55/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4511 - accuracy: 0.8118 - val_loss: 0.4114 - val_accuracy: 0.8329\n",
      "Epoch 56/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4516 - accuracy: 0.8074 - val_loss: 0.4115 - val_accuracy: 0.8247\n",
      "Epoch 57/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4528 - accuracy: 0.8075 - val_loss: 0.4007 - val_accuracy: 0.8240\n",
      "Epoch 58/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4435 - accuracy: 0.8152 - val_loss: 0.3915 - val_accuracy: 0.8356\n",
      "Epoch 59/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4387 - accuracy: 0.8171 - val_loss: 0.4267 - val_accuracy: 0.8179\n",
      "Epoch 60/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4405 - accuracy: 0.8094 - val_loss: 0.4045 - val_accuracy: 0.8397\n",
      "Epoch 61/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4422 - accuracy: 0.8153 - val_loss: 0.4115 - val_accuracy: 0.8342\n",
      "Epoch 62/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4391 - accuracy: 0.8147 - val_loss: 0.4004 - val_accuracy: 0.8410\n",
      "Epoch 63/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4502 - accuracy: 0.8115 - val_loss: 0.3922 - val_accuracy: 0.8342\n",
      "Epoch 64/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4367 - accuracy: 0.8144 - val_loss: 0.3968 - val_accuracy: 0.8383\n",
      "Epoch 65/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4402 - accuracy: 0.8178 - val_loss: 0.4089 - val_accuracy: 0.8329\n",
      "Epoch 66/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4514 - accuracy: 0.8070 - val_loss: 0.4081 - val_accuracy: 0.8247\n",
      "Epoch 67/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4382 - accuracy: 0.8128 - val_loss: 0.4000 - val_accuracy: 0.8336\n",
      "Epoch 68/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4436 - accuracy: 0.8112 - val_loss: 0.4431 - val_accuracy: 0.8159\n",
      "Epoch 69/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4394 - accuracy: 0.8107 - val_loss: 0.3986 - val_accuracy: 0.8288\n",
      "Epoch 70/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4274 - accuracy: 0.8146 - val_loss: 0.3903 - val_accuracy: 0.8465\n",
      "Epoch 71/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4432 - accuracy: 0.8135 - val_loss: 0.3900 - val_accuracy: 0.8322\n",
      "Epoch 72/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4481 - accuracy: 0.8065 - val_loss: 0.4030 - val_accuracy: 0.8329\n",
      "Epoch 73/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4475 - accuracy: 0.8114 - val_loss: 0.4229 - val_accuracy: 0.8240\n",
      "Epoch 74/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4305 - accuracy: 0.8181 - val_loss: 0.4021 - val_accuracy: 0.8363\n",
      "Epoch 75/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4254 - accuracy: 0.8180 - val_loss: 0.4081 - val_accuracy: 0.8322\n",
      "Epoch 76/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4408 - accuracy: 0.8068 - val_loss: 0.4202 - val_accuracy: 0.8247\n",
      "Epoch 77/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4338 - accuracy: 0.8164 - val_loss: 0.3967 - val_accuracy: 0.8390\n",
      "Epoch 78/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4334 - accuracy: 0.8173 - val_loss: 0.3920 - val_accuracy: 0.8417\n",
      "Epoch 79/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4338 - accuracy: 0.8216 - val_loss: 0.4028 - val_accuracy: 0.8356\n",
      "Epoch 80/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4222 - accuracy: 0.8202 - val_loss: 0.3986 - val_accuracy: 0.8376\n",
      "Epoch 81/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4242 - accuracy: 0.8236 - val_loss: 0.3865 - val_accuracy: 0.8410\n",
      "Epoch 82/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4306 - accuracy: 0.8181 - val_loss: 0.4008 - val_accuracy: 0.8451\n",
      "Epoch 83/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4403 - accuracy: 0.8135 - val_loss: 0.3869 - val_accuracy: 0.8465\n",
      "Epoch 84/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4257 - accuracy: 0.8180 - val_loss: 0.4114 - val_accuracy: 0.8315\n",
      "Epoch 85/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4310 - accuracy: 0.8141 - val_loss: 0.3980 - val_accuracy: 0.8342\n",
      "Epoch 86/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4355 - accuracy: 0.8149 - val_loss: 0.3873 - val_accuracy: 0.8444\n",
      "Epoch 87/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4278 - accuracy: 0.8215 - val_loss: 0.3843 - val_accuracy: 0.8458\n",
      "Epoch 88/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4161 - accuracy: 0.8256 - val_loss: 0.3937 - val_accuracy: 0.8336\n",
      "Epoch 89/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4238 - accuracy: 0.8186 - val_loss: 0.3993 - val_accuracy: 0.8336\n",
      "Epoch 90/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4184 - accuracy: 0.8199 - val_loss: 0.4107 - val_accuracy: 0.8173\n",
      "Epoch 91/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4273 - accuracy: 0.8169 - val_loss: 0.3862 - val_accuracy: 0.8404\n",
      "Epoch 92/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4281 - accuracy: 0.8152 - val_loss: 0.3884 - val_accuracy: 0.8336\n",
      "Epoch 93/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4172 - accuracy: 0.8248 - val_loss: 0.3870 - val_accuracy: 0.8444\n",
      "Epoch 94/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4310 - accuracy: 0.8209 - val_loss: 0.3957 - val_accuracy: 0.8261\n",
      "Epoch 95/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4275 - accuracy: 0.8197 - val_loss: 0.3829 - val_accuracy: 0.8478\n",
      "Epoch 96/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4277 - accuracy: 0.8194 - val_loss: 0.3934 - val_accuracy: 0.8288\n",
      "Epoch 97/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4311 - accuracy: 0.8188 - val_loss: 0.3876 - val_accuracy: 0.8390\n",
      "Epoch 98/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4243 - accuracy: 0.8236 - val_loss: 0.4016 - val_accuracy: 0.8451\n",
      "Epoch 99/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4261 - accuracy: 0.8172 - val_loss: 0.3979 - val_accuracy: 0.8302\n",
      "Epoch 100/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.4243 - accuracy: 0.8169 - val_loss: 0.3942 - val_accuracy: 0.8424\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4142 - accuracy: 0.8220\n",
      "Test Loss: 0.4142\n",
      "Test Accuracy: 82.20%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_texts, train_labels,\n",
    "    epochs=100,  # You can adjust the number of epochs\n",
    "    batch_size=64,\n",
    "    validation_data=(val_texts, val_labels)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(test_texts, test_labels)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_3 (KerasLayer)  (None, 20)                400020    \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               2688      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 411159 (1.57 MB)\n",
      "Trainable params: 411159 (1.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Word2Vec model from TensorFlow Hub\n",
    "embedding_model_url = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding_model_url, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "# Define your feed forward neural network\n",
    "model = tf.keras.Sequential([\n",
    "    hub_layer,  # The Word2Vec embedding layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(authors), activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 2s 7ms/step - loss: 1.0057 - accuracy: 0.5134 - val_loss: 0.8696 - val_accuracy: 0.5965\n",
      "Epoch 2/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.8603 - accuracy: 0.6063 - val_loss: 0.7125 - val_accuracy: 0.7058\n",
      "Epoch 3/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.7074 - accuracy: 0.6972 - val_loss: 0.5547 - val_accuracy: 0.7724\n",
      "Epoch 4/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.5692 - accuracy: 0.7598 - val_loss: 0.4653 - val_accuracy: 0.8010\n",
      "Epoch 5/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.4698 - accuracy: 0.8076 - val_loss: 0.4227 - val_accuracy: 0.8071\n",
      "Epoch 6/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.3964 - accuracy: 0.8404 - val_loss: 0.3994 - val_accuracy: 0.8315\n",
      "Epoch 7/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.3356 - accuracy: 0.8640 - val_loss: 0.3765 - val_accuracy: 0.8329\n",
      "Epoch 8/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.2983 - accuracy: 0.8806 - val_loss: 0.3733 - val_accuracy: 0.8451\n",
      "Epoch 9/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.2608 - accuracy: 0.8924 - val_loss: 0.3804 - val_accuracy: 0.8478\n",
      "Epoch 10/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.2404 - accuracy: 0.8989 - val_loss: 0.4011 - val_accuracy: 0.8478\n",
      "Epoch 11/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.2123 - accuracy: 0.9129 - val_loss: 0.4116 - val_accuracy: 0.8539\n",
      "Epoch 12/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1895 - accuracy: 0.9212 - val_loss: 0.4077 - val_accuracy: 0.8526\n",
      "Epoch 13/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1784 - accuracy: 0.9205 - val_loss: 0.4072 - val_accuracy: 0.8614\n",
      "Epoch 14/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1611 - accuracy: 0.9272 - val_loss: 0.4603 - val_accuracy: 0.8553\n",
      "Epoch 15/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1460 - accuracy: 0.9350 - val_loss: 0.4826 - val_accuracy: 0.8499\n",
      "Epoch 16/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1394 - accuracy: 0.9386 - val_loss: 0.5147 - val_accuracy: 0.8533\n",
      "Epoch 17/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1273 - accuracy: 0.9394 - val_loss: 0.5322 - val_accuracy: 0.8580\n",
      "Epoch 18/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1213 - accuracy: 0.9423 - val_loss: 0.5722 - val_accuracy: 0.8553\n",
      "Epoch 19/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1128 - accuracy: 0.9481 - val_loss: 0.5974 - val_accuracy: 0.8539\n",
      "Epoch 20/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1078 - accuracy: 0.9461 - val_loss: 0.6192 - val_accuracy: 0.8533\n",
      "Epoch 21/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1045 - accuracy: 0.9488 - val_loss: 0.6921 - val_accuracy: 0.8492\n",
      "Epoch 22/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1008 - accuracy: 0.9477 - val_loss: 0.6805 - val_accuracy: 0.8533\n",
      "Epoch 23/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.1012 - accuracy: 0.9508 - val_loss: 0.6992 - val_accuracy: 0.8512\n",
      "Epoch 24/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0968 - accuracy: 0.9534 - val_loss: 0.7358 - val_accuracy: 0.8471\n",
      "Epoch 25/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0915 - accuracy: 0.9522 - val_loss: 0.7445 - val_accuracy: 0.8505\n",
      "Epoch 26/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0931 - accuracy: 0.9526 - val_loss: 0.7757 - val_accuracy: 0.8512\n",
      "Epoch 27/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0883 - accuracy: 0.9558 - val_loss: 0.8248 - val_accuracy: 0.8383\n",
      "Epoch 28/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0863 - accuracy: 0.9543 - val_loss: 0.8839 - val_accuracy: 0.8471\n",
      "Epoch 29/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0859 - accuracy: 0.9546 - val_loss: 0.8707 - val_accuracy: 0.8397\n",
      "Epoch 30/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0828 - accuracy: 0.9567 - val_loss: 0.9439 - val_accuracy: 0.8485\n",
      "Epoch 31/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0835 - accuracy: 0.9557 - val_loss: 0.9612 - val_accuracy: 0.8478\n",
      "Epoch 32/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0825 - accuracy: 0.9559 - val_loss: 0.9432 - val_accuracy: 0.8438\n",
      "Epoch 33/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0827 - accuracy: 0.9563 - val_loss: 0.9852 - val_accuracy: 0.8492\n",
      "Epoch 34/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0784 - accuracy: 0.9566 - val_loss: 0.9968 - val_accuracy: 0.8397\n",
      "Epoch 35/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0804 - accuracy: 0.9565 - val_loss: 1.0008 - val_accuracy: 0.8451\n",
      "Epoch 36/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0794 - accuracy: 0.9568 - val_loss: 1.0769 - val_accuracy: 0.8390\n",
      "Epoch 37/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0779 - accuracy: 0.9583 - val_loss: 1.0700 - val_accuracy: 0.8404\n",
      "Epoch 38/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0768 - accuracy: 0.9571 - val_loss: 1.0670 - val_accuracy: 0.8519\n",
      "Epoch 39/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0759 - accuracy: 0.9594 - val_loss: 1.0668 - val_accuracy: 0.8438\n",
      "Epoch 40/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0770 - accuracy: 0.9596 - val_loss: 1.1253 - val_accuracy: 0.8404\n",
      "Epoch 41/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0768 - accuracy: 0.9574 - val_loss: 1.0537 - val_accuracy: 0.8546\n",
      "Epoch 42/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0745 - accuracy: 0.9581 - val_loss: 1.1282 - val_accuracy: 0.8451\n",
      "Epoch 43/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0761 - accuracy: 0.9578 - val_loss: 1.1194 - val_accuracy: 0.8390\n",
      "Epoch 44/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0745 - accuracy: 0.9588 - val_loss: 1.2020 - val_accuracy: 0.8499\n",
      "Epoch 45/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0810 - accuracy: 0.9579 - val_loss: 1.1194 - val_accuracy: 0.8492\n",
      "Epoch 46/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0739 - accuracy: 0.9583 - val_loss: 1.1257 - val_accuracy: 0.8505\n",
      "Epoch 47/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0741 - accuracy: 0.9598 - val_loss: 1.1524 - val_accuracy: 0.8451\n",
      "Epoch 48/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0702 - accuracy: 0.9598 - val_loss: 1.2369 - val_accuracy: 0.8438\n",
      "Epoch 49/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0716 - accuracy: 0.9608 - val_loss: 1.2675 - val_accuracy: 0.8471\n",
      "Epoch 50/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0695 - accuracy: 0.9608 - val_loss: 1.4274 - val_accuracy: 0.8431\n",
      "Epoch 51/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0721 - accuracy: 0.9614 - val_loss: 1.2673 - val_accuracy: 0.8424\n",
      "Epoch 52/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0736 - accuracy: 0.9587 - val_loss: 1.2379 - val_accuracy: 0.8431\n",
      "Epoch 53/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0686 - accuracy: 0.9628 - val_loss: 1.3753 - val_accuracy: 0.8485\n",
      "Epoch 54/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0681 - accuracy: 0.9622 - val_loss: 1.4232 - val_accuracy: 0.8404\n",
      "Epoch 55/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0706 - accuracy: 0.9606 - val_loss: 1.3478 - val_accuracy: 0.8499\n",
      "Epoch 56/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0688 - accuracy: 0.9608 - val_loss: 1.3209 - val_accuracy: 0.8451\n",
      "Epoch 57/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0692 - accuracy: 0.9607 - val_loss: 1.3760 - val_accuracy: 0.8383\n",
      "Epoch 58/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0709 - accuracy: 0.9616 - val_loss: 1.3739 - val_accuracy: 0.8376\n",
      "Epoch 59/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0689 - accuracy: 0.9620 - val_loss: 1.5006 - val_accuracy: 0.8315\n",
      "Epoch 60/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0683 - accuracy: 0.9615 - val_loss: 1.5041 - val_accuracy: 0.8329\n",
      "Epoch 61/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0647 - accuracy: 0.9623 - val_loss: 1.6036 - val_accuracy: 0.8336\n",
      "Epoch 62/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0687 - accuracy: 0.9618 - val_loss: 1.5654 - val_accuracy: 0.8383\n",
      "Epoch 63/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0685 - accuracy: 0.9613 - val_loss: 1.5323 - val_accuracy: 0.8376\n",
      "Epoch 64/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0668 - accuracy: 0.9620 - val_loss: 1.6089 - val_accuracy: 0.8322\n",
      "Epoch 65/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0669 - accuracy: 0.9613 - val_loss: 1.5588 - val_accuracy: 0.8370\n",
      "Epoch 66/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0672 - accuracy: 0.9615 - val_loss: 1.5472 - val_accuracy: 0.8329\n",
      "Epoch 67/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0687 - accuracy: 0.9611 - val_loss: 1.5904 - val_accuracy: 0.8315\n",
      "Epoch 68/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0671 - accuracy: 0.9654 - val_loss: 1.7059 - val_accuracy: 0.8315\n",
      "Epoch 69/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0657 - accuracy: 0.9631 - val_loss: 1.7403 - val_accuracy: 0.8322\n",
      "Epoch 70/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0645 - accuracy: 0.9640 - val_loss: 1.7892 - val_accuracy: 0.8349\n",
      "Epoch 71/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0639 - accuracy: 0.9645 - val_loss: 1.8375 - val_accuracy: 0.8315\n",
      "Epoch 72/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0651 - accuracy: 0.9640 - val_loss: 1.7076 - val_accuracy: 0.8356\n",
      "Epoch 73/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0642 - accuracy: 0.9640 - val_loss: 1.8022 - val_accuracy: 0.8308\n",
      "Epoch 74/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0653 - accuracy: 0.9625 - val_loss: 1.7207 - val_accuracy: 0.8370\n",
      "Epoch 75/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0657 - accuracy: 0.9653 - val_loss: 1.9186 - val_accuracy: 0.8281\n",
      "Epoch 76/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0630 - accuracy: 0.9659 - val_loss: 1.8415 - val_accuracy: 0.8383\n",
      "Epoch 77/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0645 - accuracy: 0.9642 - val_loss: 1.9064 - val_accuracy: 0.8404\n",
      "Epoch 78/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0609 - accuracy: 0.9668 - val_loss: 1.9697 - val_accuracy: 0.8329\n",
      "Epoch 79/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0608 - accuracy: 0.9662 - val_loss: 2.1107 - val_accuracy: 0.8342\n",
      "Epoch 80/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0654 - accuracy: 0.9662 - val_loss: 2.0198 - val_accuracy: 0.8295\n",
      "Epoch 81/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0677 - accuracy: 0.9645 - val_loss: 1.9078 - val_accuracy: 0.8370\n",
      "Epoch 82/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0622 - accuracy: 0.9640 - val_loss: 1.9134 - val_accuracy: 0.8315\n",
      "Epoch 83/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0632 - accuracy: 0.9648 - val_loss: 1.8850 - val_accuracy: 0.8376\n",
      "Epoch 84/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0586 - accuracy: 0.9681 - val_loss: 2.1052 - val_accuracy: 0.8342\n",
      "Epoch 85/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0602 - accuracy: 0.9679 - val_loss: 2.1619 - val_accuracy: 0.8322\n",
      "Epoch 86/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0634 - accuracy: 0.9658 - val_loss: 2.0304 - val_accuracy: 0.8390\n",
      "Epoch 87/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0595 - accuracy: 0.9668 - val_loss: 2.2248 - val_accuracy: 0.8363\n",
      "Epoch 88/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0624 - accuracy: 0.9668 - val_loss: 2.0472 - val_accuracy: 0.8370\n",
      "Epoch 89/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0593 - accuracy: 0.9672 - val_loss: 2.1576 - val_accuracy: 0.8336\n",
      "Epoch 90/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0570 - accuracy: 0.9698 - val_loss: 2.1796 - val_accuracy: 0.8363\n",
      "Epoch 91/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0594 - accuracy: 0.9678 - val_loss: 2.2652 - val_accuracy: 0.8376\n",
      "Epoch 92/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0600 - accuracy: 0.9681 - val_loss: 2.1682 - val_accuracy: 0.8397\n",
      "Epoch 93/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0621 - accuracy: 0.9673 - val_loss: 2.1438 - val_accuracy: 0.8336\n",
      "Epoch 94/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0631 - accuracy: 0.9676 - val_loss: 2.0018 - val_accuracy: 0.8390\n",
      "Epoch 95/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0585 - accuracy: 0.9704 - val_loss: 2.0926 - val_accuracy: 0.8390\n",
      "Epoch 96/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0577 - accuracy: 0.9697 - val_loss: 2.1590 - val_accuracy: 0.8390\n",
      "Epoch 97/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0567 - accuracy: 0.9690 - val_loss: 2.1906 - val_accuracy: 0.8417\n",
      "Epoch 98/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0561 - accuracy: 0.9716 - val_loss: 2.3598 - val_accuracy: 0.8349\n",
      "Epoch 99/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0567 - accuracy: 0.9704 - val_loss: 2.2618 - val_accuracy: 0.8322\n",
      "Epoch 100/100\n",
      "184/184 [==============================] - 1s 6ms/step - loss: 0.0542 - accuracy: 0.9707 - val_loss: 2.3772 - val_accuracy: 0.8329\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 2.1130 - accuracy: 0.8370\n",
      "Test Loss: 2.1130\n",
      "Test Accuracy: 83.70%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_texts, train_labels,\n",
    "    epochs=100,  # You can adjust the number of epochs\n",
    "    batch_size=64,\n",
    "    validation_data=(val_texts, val_labels)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(test_texts, test_labels)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_4 (KerasLayer)  (None, 250)               252343750 \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               32128     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252384329 (962.77 MB)\n",
      "Trainable params: 40579 (158.51 KB)\n",
      "Non-trainable params: 252343750 (962.62 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Word2Vec model from TensorFlow Hub\n",
    "embedding_model_url = \"https://tfhub.dev/google/Wiki-words-250/2\"\n",
    "hub_layer = hub.KerasLayer(embedding_model_url, input_shape=[], dtype=tf.string, trainable=False)\n",
    "\n",
    "# Define your feed forward neural network\n",
    "model = tf.keras.Sequential([\n",
    "    hub_layer,  # The Word2Vec embedding layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(authors), activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "184/184 [==============================] - 1s 5ms/step - loss: 0.9060 - accuracy: 0.5748 - val_loss: 0.7556 - val_accuracy: 0.6508\n",
      "Epoch 2/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.7873 - accuracy: 0.6430 - val_loss: 0.6768 - val_accuracy: 0.6882\n",
      "Epoch 3/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.7286 - accuracy: 0.6776 - val_loss: 0.6230 - val_accuracy: 0.7296\n",
      "Epoch 4/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.6890 - accuracy: 0.6966 - val_loss: 0.5952 - val_accuracy: 0.7432\n",
      "Epoch 5/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.6568 - accuracy: 0.7135 - val_loss: 0.5614 - val_accuracy: 0.7643\n",
      "Epoch 6/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.6403 - accuracy: 0.7216 - val_loss: 0.5689 - val_accuracy: 0.7683\n",
      "Epoch 7/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.6159 - accuracy: 0.7323 - val_loss: 0.5279 - val_accuracy: 0.7880\n",
      "Epoch 8/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.6086 - accuracy: 0.7342 - val_loss: 0.5268 - val_accuracy: 0.7711\n",
      "Epoch 9/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5941 - accuracy: 0.7462 - val_loss: 0.5202 - val_accuracy: 0.7799\n",
      "Epoch 10/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5885 - accuracy: 0.7431 - val_loss: 0.5071 - val_accuracy: 0.7901\n",
      "Epoch 11/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5776 - accuracy: 0.7542 - val_loss: 0.5040 - val_accuracy: 0.7901\n",
      "Epoch 12/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5737 - accuracy: 0.7490 - val_loss: 0.5054 - val_accuracy: 0.7846\n",
      "Epoch 13/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5694 - accuracy: 0.7589 - val_loss: 0.5004 - val_accuracy: 0.7846\n",
      "Epoch 14/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5654 - accuracy: 0.7548 - val_loss: 0.4941 - val_accuracy: 0.7887\n",
      "Epoch 15/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5573 - accuracy: 0.7599 - val_loss: 0.4913 - val_accuracy: 0.7996\n",
      "Epoch 16/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5470 - accuracy: 0.7675 - val_loss: 0.5050 - val_accuracy: 0.7833\n",
      "Epoch 17/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5467 - accuracy: 0.7646 - val_loss: 0.4874 - val_accuracy: 0.7955\n",
      "Epoch 18/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5385 - accuracy: 0.7654 - val_loss: 0.4915 - val_accuracy: 0.7853\n",
      "Epoch 19/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5398 - accuracy: 0.7679 - val_loss: 0.4813 - val_accuracy: 0.7955\n",
      "Epoch 20/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5398 - accuracy: 0.7673 - val_loss: 0.4857 - val_accuracy: 0.8050\n",
      "Epoch 21/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5301 - accuracy: 0.7716 - val_loss: 0.4720 - val_accuracy: 0.7996\n",
      "Epoch 22/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5275 - accuracy: 0.7741 - val_loss: 0.5055 - val_accuracy: 0.7806\n",
      "Epoch 23/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5261 - accuracy: 0.7757 - val_loss: 0.4767 - val_accuracy: 0.7935\n",
      "Epoch 24/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5221 - accuracy: 0.7757 - val_loss: 0.4765 - val_accuracy: 0.7948\n",
      "Epoch 25/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5213 - accuracy: 0.7776 - val_loss: 0.4831 - val_accuracy: 0.7921\n",
      "Epoch 26/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5212 - accuracy: 0.7717 - val_loss: 0.4827 - val_accuracy: 0.7982\n",
      "Epoch 27/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5128 - accuracy: 0.7829 - val_loss: 0.4685 - val_accuracy: 0.7982\n",
      "Epoch 28/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5123 - accuracy: 0.7800 - val_loss: 0.4713 - val_accuracy: 0.7894\n",
      "Epoch 29/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5142 - accuracy: 0.7812 - val_loss: 0.4676 - val_accuracy: 0.7976\n",
      "Epoch 30/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5131 - accuracy: 0.7801 - val_loss: 0.4780 - val_accuracy: 0.7914\n",
      "Epoch 31/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5071 - accuracy: 0.7814 - val_loss: 0.4646 - val_accuracy: 0.8064\n",
      "Epoch 32/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5098 - accuracy: 0.7806 - val_loss: 0.4784 - val_accuracy: 0.7942\n",
      "Epoch 33/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5153 - accuracy: 0.7764 - val_loss: 0.4746 - val_accuracy: 0.7996\n",
      "Epoch 34/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5090 - accuracy: 0.7809 - val_loss: 0.4610 - val_accuracy: 0.8091\n",
      "Epoch 35/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5031 - accuracy: 0.7838 - val_loss: 0.4588 - val_accuracy: 0.8071\n",
      "Epoch 36/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.5013 - accuracy: 0.7853 - val_loss: 0.4545 - val_accuracy: 0.8091\n",
      "Epoch 37/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4991 - accuracy: 0.7864 - val_loss: 0.4632 - val_accuracy: 0.8057\n",
      "Epoch 38/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4958 - accuracy: 0.7852 - val_loss: 0.4582 - val_accuracy: 0.8064\n",
      "Epoch 39/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4847 - accuracy: 0.7907 - val_loss: 0.4622 - val_accuracy: 0.8010\n",
      "Epoch 40/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4929 - accuracy: 0.7884 - val_loss: 0.4642 - val_accuracy: 0.7989\n",
      "Epoch 41/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4844 - accuracy: 0.7929 - val_loss: 0.4567 - val_accuracy: 0.8118\n",
      "Epoch 42/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4920 - accuracy: 0.7898 - val_loss: 0.4518 - val_accuracy: 0.8077\n",
      "Epoch 43/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4974 - accuracy: 0.7881 - val_loss: 0.4936 - val_accuracy: 0.7962\n",
      "Epoch 44/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4914 - accuracy: 0.7860 - val_loss: 0.4529 - val_accuracy: 0.8084\n",
      "Epoch 45/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4841 - accuracy: 0.7892 - val_loss: 0.4455 - val_accuracy: 0.8023\n",
      "Epoch 46/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4803 - accuracy: 0.7935 - val_loss: 0.4507 - val_accuracy: 0.8098\n",
      "Epoch 47/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4818 - accuracy: 0.7908 - val_loss: 0.4636 - val_accuracy: 0.8050\n",
      "Epoch 48/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4842 - accuracy: 0.7851 - val_loss: 0.4523 - val_accuracy: 0.8105\n",
      "Epoch 49/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4809 - accuracy: 0.7954 - val_loss: 0.4524 - val_accuracy: 0.8071\n",
      "Epoch 50/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4853 - accuracy: 0.7934 - val_loss: 0.4513 - val_accuracy: 0.8111\n",
      "Epoch 51/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4780 - accuracy: 0.7928 - val_loss: 0.4489 - val_accuracy: 0.8125\n",
      "Epoch 52/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4846 - accuracy: 0.7878 - val_loss: 0.4474 - val_accuracy: 0.8118\n",
      "Epoch 53/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4869 - accuracy: 0.7903 - val_loss: 0.4524 - val_accuracy: 0.8111\n",
      "Epoch 54/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4830 - accuracy: 0.7936 - val_loss: 0.4505 - val_accuracy: 0.8173\n",
      "Epoch 55/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4763 - accuracy: 0.7960 - val_loss: 0.4508 - val_accuracy: 0.8152\n",
      "Epoch 56/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4875 - accuracy: 0.7925 - val_loss: 0.4490 - val_accuracy: 0.8098\n",
      "Epoch 57/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4748 - accuracy: 0.7966 - val_loss: 0.4494 - val_accuracy: 0.8098\n",
      "Epoch 58/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4681 - accuracy: 0.7987 - val_loss: 0.4423 - val_accuracy: 0.8152\n",
      "Epoch 59/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4767 - accuracy: 0.7945 - val_loss: 0.4548 - val_accuracy: 0.8098\n",
      "Epoch 60/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4753 - accuracy: 0.7953 - val_loss: 0.4582 - val_accuracy: 0.8030\n",
      "Epoch 61/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4711 - accuracy: 0.7963 - val_loss: 0.4510 - val_accuracy: 0.8084\n",
      "Epoch 62/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4687 - accuracy: 0.8008 - val_loss: 0.4467 - val_accuracy: 0.8064\n",
      "Epoch 63/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4774 - accuracy: 0.7957 - val_loss: 0.4488 - val_accuracy: 0.8105\n",
      "Epoch 64/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4658 - accuracy: 0.8009 - val_loss: 0.4578 - val_accuracy: 0.8111\n",
      "Epoch 65/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4699 - accuracy: 0.7963 - val_loss: 0.4477 - val_accuracy: 0.8098\n",
      "Epoch 66/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4671 - accuracy: 0.8026 - val_loss: 0.4530 - val_accuracy: 0.8152\n",
      "Epoch 67/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4729 - accuracy: 0.7959 - val_loss: 0.4516 - val_accuracy: 0.8125\n",
      "Epoch 68/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4566 - accuracy: 0.8044 - val_loss: 0.4405 - val_accuracy: 0.8193\n",
      "Epoch 69/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4709 - accuracy: 0.7994 - val_loss: 0.4516 - val_accuracy: 0.8084\n",
      "Epoch 70/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4745 - accuracy: 0.7945 - val_loss: 0.4534 - val_accuracy: 0.8111\n",
      "Epoch 71/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4688 - accuracy: 0.8004 - val_loss: 0.4436 - val_accuracy: 0.8125\n",
      "Epoch 72/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4509 - accuracy: 0.8083 - val_loss: 0.4384 - val_accuracy: 0.8173\n",
      "Epoch 73/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4630 - accuracy: 0.7983 - val_loss: 0.4422 - val_accuracy: 0.8139\n",
      "Epoch 74/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4658 - accuracy: 0.8007 - val_loss: 0.4425 - val_accuracy: 0.8077\n",
      "Epoch 75/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4686 - accuracy: 0.7953 - val_loss: 0.4463 - val_accuracy: 0.8152\n",
      "Epoch 76/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4722 - accuracy: 0.7945 - val_loss: 0.4425 - val_accuracy: 0.8050\n",
      "Epoch 77/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4630 - accuracy: 0.8039 - val_loss: 0.4391 - val_accuracy: 0.8016\n",
      "Epoch 78/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4662 - accuracy: 0.7995 - val_loss: 0.4452 - val_accuracy: 0.8098\n",
      "Epoch 79/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4663 - accuracy: 0.7950 - val_loss: 0.4385 - val_accuracy: 0.8173\n",
      "Epoch 80/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4615 - accuracy: 0.8053 - val_loss: 0.4408 - val_accuracy: 0.8125\n",
      "Epoch 81/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4585 - accuracy: 0.8015 - val_loss: 0.4459 - val_accuracy: 0.8118\n",
      "Epoch 82/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4497 - accuracy: 0.8041 - val_loss: 0.4517 - val_accuracy: 0.8071\n",
      "Epoch 83/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4581 - accuracy: 0.8037 - val_loss: 0.4400 - val_accuracy: 0.8145\n",
      "Epoch 84/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4577 - accuracy: 0.8004 - val_loss: 0.4430 - val_accuracy: 0.8173\n",
      "Epoch 85/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4533 - accuracy: 0.8039 - val_loss: 0.4380 - val_accuracy: 0.8139\n",
      "Epoch 86/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4616 - accuracy: 0.8059 - val_loss: 0.4396 - val_accuracy: 0.8125\n",
      "Epoch 87/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4463 - accuracy: 0.8127 - val_loss: 0.4592 - val_accuracy: 0.8057\n",
      "Epoch 88/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4463 - accuracy: 0.8077 - val_loss: 0.4447 - val_accuracy: 0.8091\n",
      "Epoch 89/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4489 - accuracy: 0.8112 - val_loss: 0.4408 - val_accuracy: 0.8125\n",
      "Epoch 90/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4631 - accuracy: 0.8026 - val_loss: 0.4437 - val_accuracy: 0.8057\n",
      "Epoch 91/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4572 - accuracy: 0.8045 - val_loss: 0.4390 - val_accuracy: 0.8139\n",
      "Epoch 92/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4452 - accuracy: 0.8130 - val_loss: 0.4367 - val_accuracy: 0.8145\n",
      "Epoch 93/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4439 - accuracy: 0.8113 - val_loss: 0.4510 - val_accuracy: 0.8016\n",
      "Epoch 94/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4510 - accuracy: 0.8082 - val_loss: 0.4659 - val_accuracy: 0.8064\n",
      "Epoch 95/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4423 - accuracy: 0.8080 - val_loss: 0.4368 - val_accuracy: 0.8145\n",
      "Epoch 96/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4405 - accuracy: 0.8130 - val_loss: 0.4290 - val_accuracy: 0.8227\n",
      "Epoch 97/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4514 - accuracy: 0.8051 - val_loss: 0.4354 - val_accuracy: 0.8111\n",
      "Epoch 98/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4521 - accuracy: 0.8038 - val_loss: 0.4377 - val_accuracy: 0.8118\n",
      "Epoch 99/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4372 - accuracy: 0.8131 - val_loss: 0.4357 - val_accuracy: 0.8159\n",
      "Epoch 100/100\n",
      "184/184 [==============================] - 1s 4ms/step - loss: 0.4411 - accuracy: 0.8169 - val_loss: 0.4346 - val_accuracy: 0.8173\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.8003\n",
      "Test Loss: 0.4423\n",
      "Test Accuracy: 80.03%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_texts, train_labels,\n",
    "    epochs=100,  # You can adjust the number of epochs\n",
    "    batch_size=64,\n",
    "    validation_data=(val_texts, val_labels)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(test_texts, test_labels)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que el desempeño de distintos modelos preentrenados \"word2vec\", es menor al desempeño usando embeddings entrenedaos directamente en el texto objetivo. esto puede ser debido a que estos embeddigns están diseñados para generalizar en muchos otros contextos, lo que remueve parcialmente poder de representación que puede explotar el modelo entrenado directamente sobre el corpus a analizar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
