{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperación ranqueada y vectorización de documentos (RRDV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vuelve a crear el indice invertido junto a sus frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción completada\n"
     ]
    }
   ],
   "source": [
    "# Se extraen los datos de los archivos comprimidos\n",
    "\n",
    "# Specify the paths to the compressed files and the target directory\n",
    "compressed_files = ['docs-raw-texts.zip', 'queries-raw-texts.zip']\n",
    "\n",
    "num_documents = 0\n",
    "# Extract files from each compressed file\n",
    "for compressed_file in compressed_files:\n",
    "    with zipfile.ZipFile(compressed_file, 'r') as zip_ref:\n",
    "        folder_name = os.path.splitext(compressed_file)[0]  # Remove the \".zip\" extension\n",
    "        target_folder = os.path.join(folder_name)\n",
    "        \n",
    "        if not os.path.exists(target_folder):\n",
    "            # Create the folder within the target directory\n",
    "            os.mkdir(target_folder)\n",
    "\n",
    "        \n",
    "            # Extract all files to the target folder\n",
    "            zip_ref.extractall(target_folder)\n",
    "\n",
    "print(\"Extracción completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths a directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios que contienen los archivos necesarios, cambiar acá si es necesario\n",
    "xml_files_directory = 'docs-raw-texts'\n",
    "\n",
    "relevance_judgments_directory = \"relevance-judgments.tsv\"\n",
    "\n",
    "queries_directory = \"queries-raw-texts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the NLTK stopwords resource\n",
    "nltk.download('stopwords')\n",
    "# NLTK setup\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función de preprocesamiento, se usará para todos los inputs al modelo (queries y documentos)\n",
    "def preprocess_text(text):\n",
    "    text.strip().lower() # normalización del texto, todo en minúscula y se quitan espacios innecesarios.\n",
    "    tokens = tokenizer.tokenize(text) #tokenización por espacio\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words] # eliminación de palabras vacias y stemming\n",
    "    return tokens # retorna lista con el texto tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_text(xml_path: str) -> str:\n",
    "    \"\"\"Extracts raw text from a .naf file.\n",
    "\n",
    "    Args:\n",
    "        xml_path (str): Path to the .naf file.\n",
    "\n",
    "    Returns:\n",
    "        str: The raw text from the .naf file.\n",
    "    \"\"\"\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract content from the XML\n",
    "    raw_text = root.find('raw').text\n",
    "\n",
    "    # Check if the title is present in the raw text\n",
    "    title = root.find(\".//nafHeader/fileDesc\").get(\"title\")\n",
    "    if title and title not in raw_text:\n",
    "        raw_text = title + \", \" + raw_text\n",
    "\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index created.\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the inverted index (term -> list of documents)\n",
    "inverted_index = {}\n",
    "\n",
    "# Dictionary to store term frequencies per document (term -> {document: frequency})\n",
    "term_freq_per_document = {}\n",
    "\n",
    "# Iterate over XML files in the directory\n",
    "for filename in os.listdir(xml_files_directory):\n",
    "    if filename.endswith('.naf'):\n",
    "        xml_path = os.path.join(xml_files_directory, filename)\n",
    "        content = extract_raw_text(xml_path)\n",
    "        # Preprocess the content\n",
    "        preprocessed_tokens = preprocess_text(content)\n",
    "        \n",
    "        # Create the inverted index and update term frequencies per document\n",
    "        for term in preprocessed_tokens:\n",
    "            if term in inverted_index:\n",
    "                if filename not in inverted_index[term]:\n",
    "                    inverted_index[term].append(filename)\n",
    "            else:\n",
    "                inverted_index[term] = [filename]\n",
    "            \n",
    "            if term in term_freq_per_document:\n",
    "                if filename in term_freq_per_document[term]:\n",
    "                    term_freq_per_document[term][filename] += 1\n",
    "                else:\n",
    "                    term_freq_per_document[term][filename] = 1\n",
    "            else:\n",
    "                term_freq_per_document[term] = {filename: 1}\n",
    "\n",
    "print(\"Inverted index created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "TF\\text{-}IDF_{t,d} = \\log(1 + \\text{TF}_{t,d}) \\times \\log \\left( \\frac{N}{\\text{DF}_t} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vector for query: William Beaumont is Confused by human physiology\n",
      "[0.24710288 0.75854381 0.22326695 ... 0.         0.         0.        ]\n",
      "elements: 13631\n",
      "non-zero elements: 5\n"
     ]
    }
   ],
   "source": [
    "def preprocess_input(query_string):\n",
    "    query_terms = preprocess_text(query_string)\n",
    "    term_frequency = {}\n",
    "\n",
    "    for term in query_terms:\n",
    "        if term in term_frequency:\n",
    "            term_frequency[term] += 1\n",
    "        else:\n",
    "            term_frequency[term] = 1\n",
    "    \n",
    "    return term_frequency\n",
    "\n",
    "# Function to calculate TF-IDF vector for a query\n",
    "def calculate_tfidf_vector(input, inverted_index, term_freq_per_document_=None, xml_files_directory=xml_files_directory):\n",
    "    total_documents = len(os.listdir(xml_files_directory))\n",
    "    query = preprocess_input(input)\n",
    "    term_indices = {}  # Map term indices to integer indices in the tfidf_vector\n",
    "    \n",
    "    for term_index, term in enumerate(inverted_index):\n",
    "        term_indices[term] = term_index\n",
    "    \n",
    "    tfidf_vector = np.zeros(len(inverted_index))  # Initialize a vector of zeros\n",
    "    \n",
    "    for term, term_index in inverted_index.items():\n",
    "        tf = query.get(term, 0)  # Term frequency in the query\n",
    "        df = len(term_index)\n",
    "        \n",
    "        tfidf = (np.log10(1 + tf)) * np.log10(total_documents / df)\n",
    "        index = term_indices[term]\n",
    "        tfidf_vector[index] = tfidf\n",
    "    \n",
    "    return tfidf_vector\n",
    "\n",
    "# Example usage\n",
    "query_string = \"William Beaumont is Confused by human physiology\"\n",
    "tfidf_vector = calculate_tfidf_vector(query_string, inverted_index)\n",
    "print(\"TF-IDF Vector for query:\", query_string)\n",
    "print(tfidf_vector)\n",
    "print(\"elements: {}\".format(len(tfidf_vector)))\n",
    "print(\"non-zero elements: {}\".format(sum(tfidf_vector>0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    \n",
    "    if norm_vector1 == 0 or norm_vector2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results in RRDV-consultas_resultados.tsv already exist\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve and rank documents based on cosine similarity scores\n",
    "def retrieve_and_rank_documents(query_string, inverted_index, term_freq_per_document, xml_files_directory=xml_files_directory):\n",
    "    similarity_scores = {}  # Dictionary to store cosine similarity scores\n",
    "    query_vector = calculate_tfidf_vector(query_string, inverted_index, term_freq_per_document)\n",
    "    \n",
    "    for document in os.listdir(xml_files_directory):\n",
    "        if document.endswith('.naf'):\n",
    "            document_path = os.path.join(xml_files_directory, document)\n",
    "            document_text = extract_raw_text(document_path)\n",
    "            document_vector = calculate_tfidf_vector(document_text, inverted_index, term_freq_per_document, xml_files_directory=xml_files_directory)\n",
    "            similarity = calculate_cosine_similarity(query_vector, document_vector)\n",
    "            \n",
    "            if similarity > 0:\n",
    "                similarity_scores[document[:-4]] = similarity  # Remove the \".naf\" extension\n",
    "    \n",
    "    ranked_documents = sorted(similarity_scores.keys(), key=lambda doc: similarity_scores[doc], reverse=True)\n",
    "    return ranked_documents, similarity_scores\n",
    "\n",
    "output_filename = \"RRDV-consultas_resultados.tsv\"\n",
    "\n",
    "if os.path.exists(output_filename):\n",
    "    print(\"Results in {} already exist\".format(output_filename))\n",
    "else:\n",
    "    # Write results to file\n",
    "    results_file = open(output_filename, \"w\")\n",
    "\n",
    "    # Iterate over queries\n",
    "    for query_file in os.listdir(queries_directory):\n",
    "        if query_file.endswith('.naf'):\n",
    "            query_path = os.path.join(queries_directory, query_file)\n",
    "            query_text = extract_raw_text(query_path)\n",
    "            \n",
    "            ranked_documents, similarity_scores = retrieve_and_rank_documents(query_text, inverted_index, term_freq_per_document)\n",
    "            \n",
    "            # Write results to the file\n",
    "            result_line = query_file[8:-4] + \"\\t\" + \",\".join([f\"{doc[8:]}:{similarity_scores[doc]:.6f}\" for doc in ranked_documents])\n",
    "            results_file.write(result_line + \"\\n\")\n",
    "            print(\"finished query {}\".format(query_file))\n",
    "\n",
    "    results_file.close()\n",
    "    print(\"Results written to \"+ output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics by Query:\n",
      "Query: q01\n",
      "P@M: 0.3333, R@M: 0.3333, NDCG@M: 0.9705\n",
      "\n",
      "Query: q02\n",
      "P@M: 0.5455, R@M: 0.5455, NDCG@M: 0.8576\n",
      "\n",
      "Query: q03\n",
      "P@M: 1.0000, R@M: 1.0000, NDCG@M: 0.9717\n",
      "\n",
      "Query: q04\n",
      "P@M: 0.7143, R@M: 0.7143, NDCG@M: 0.9756\n",
      "\n",
      "Query: q06\n",
      "P@M: 0.8333, R@M: 0.8333, NDCG@M: 0.8140\n",
      "\n",
      "Query: q07\n",
      "P@M: 0.2500, R@M: 0.2500, NDCG@M: 0.9853\n",
      "\n",
      "Query: q08\n",
      "P@M: 0.7500, R@M: 0.7500, NDCG@M: 0.8914\n",
      "\n",
      "Query: q09\n",
      "P@M: 0.8333, R@M: 0.8333, NDCG@M: 1.0000\n",
      "\n",
      "Query: q10\n",
      "P@M: 0.5000, R@M: 0.5000, NDCG@M: 0.8358\n",
      "\n",
      "Query: q12\n",
      "P@M: 0.7500, R@M: 0.7500, NDCG@M: 0.9891\n",
      "\n",
      "Query: q13\n",
      "P@M: 0.8000, R@M: 0.8000, NDCG@M: 0.8077\n",
      "\n",
      "Query: q14\n",
      "P@M: 0.5833, R@M: 0.5833, NDCG@M: 0.8661\n",
      "\n",
      "Query: q16\n",
      "P@M: 0.5000, R@M: 0.5000, NDCG@M: 1.0000\n",
      "\n",
      "Query: q17\n",
      "P@M: 0.7500, R@M: 0.7500, NDCG@M: 0.9281\n",
      "\n",
      "Query: q18\n",
      "P@M: 0.8571, R@M: 0.8571, NDCG@M: 0.9205\n",
      "\n",
      "Query: q19\n",
      "P@M: 0.5000, R@M: 0.5000, NDCG@M: 1.0000\n",
      "\n",
      "Query: q22\n",
      "P@M: 0.4286, R@M: 0.4286, NDCG@M: 1.0000\n",
      "\n",
      "Query: q23\n",
      "P@M: 0.2500, R@M: 0.2500, NDCG@M: 0.8071\n",
      "\n",
      "Query: q24\n",
      "P@M: 0.2000, R@M: 0.2000, NDCG@M: 1.0000\n",
      "\n",
      "Query: q25\n",
      "P@M: 0.5000, R@M: 0.5000, NDCG@M: 1.0000\n",
      "\n",
      "Query: q26\n",
      "P@M: 1.0000, R@M: 1.0000, NDCG@M: 1.0000\n",
      "\n",
      "Query: q27\n",
      "P@M: 0.3750, R@M: 0.3750, NDCG@M: 0.8556\n",
      "\n",
      "Query: q28\n",
      "P@M: 0.6667, R@M: 0.6667, NDCG@M: 1.0000\n",
      "\n",
      "Query: q29\n",
      "P@M: 0.8333, R@M: 0.8333, NDCG@M: 0.9489\n",
      "\n",
      "Query: q32\n",
      "P@M: 1.0000, R@M: 1.0000, NDCG@M: 0.8961\n",
      "\n",
      "Query: q34\n",
      "P@M: 1.0000, R@M: 1.0000, NDCG@M: 1.0000\n",
      "\n",
      "Query: q36\n",
      "P@M: 0.9000, R@M: 0.9000, NDCG@M: 0.9188\n",
      "\n",
      "Query: q37\n",
      "P@M: 0.6667, R@M: 0.6667, NDCG@M: 0.9602\n",
      "\n",
      "Query: q38\n",
      "P@M: 0.3750, R@M: 0.3750, NDCG@M: 0.9307\n",
      "\n",
      "Query: q40\n",
      "P@M: 0.8889, R@M: 0.8889, NDCG@M: 0.8878\n",
      "\n",
      "Query: q41\n",
      "P@M: 0.8571, R@M: 0.8571, NDCG@M: 0.8814\n",
      "\n",
      "Query: q42\n",
      "P@M: 0.3333, R@M: 0.3333, NDCG@M: 0.8660\n",
      "\n",
      "Query: q44\n",
      "P@M: 0.7000, R@M: 0.7000, NDCG@M: 0.8489\n",
      "\n",
      "Query: q45\n",
      "P@M: 0.7500, R@M: 0.7500, NDCG@M: 0.9560\n",
      "\n",
      "Query: q46\n",
      "P@M: 0.3333, R@M: 0.3333, NDCG@M: 0.8875\n",
      "\n",
      "MAP: 0.7394\n"
     ]
    }
   ],
   "source": [
    "from metrics import precision_at_k, recall_at_k, ndcg_at_k, mean_average_precision\n",
    "\n",
    "# filtra los resultados de las listas para solo incluir los nombres de los documentos\n",
    "def filter_result_list(input_list):\n",
    "    return[full_result[:full_result.find(\":\")] for full_result in input_list]\n",
    "\n",
    "# Load the relevance judgments from the file\n",
    "relevance_judgments_file = open(relevance_judgments_directory, \"r\")\n",
    "relevance_judgments = {}\n",
    "\n",
    "for line in relevance_judgments_file:\n",
    "    query, judgments = line.strip().split('\\t')\n",
    "    relevance_judgments[query] = judgments.split(',')\n",
    "\n",
    "relevance_judgments_file.close()\n",
    "\n",
    "# Load the query results from the file\n",
    "query_results_file = open(\"RRDV-consultas_resultados.tsv\", \"r\")\n",
    "query_results = {}\n",
    "\n",
    "for line in query_results_file:\n",
    "    query, results = line.strip().split('\\t')\n",
    "    query_results[query] = results.split(',')\n",
    "\n",
    "query_results_file.close()\n",
    "\n",
    "# Calculate metrics for each query\n",
    "metrics_by_query = {}\n",
    "\n",
    "for query in query_results:\n",
    "    query_judgments = relevance_judgments.get(query, [])\n",
    "    query_results_list = query_results[query]\n",
    "    query_results_list_filtered = filter_result_list(query_results_list)\n",
    "    query_judgments_filtered = filter_result_list(query_judgments)\n",
    "    \n",
    "    if query in relevance_judgments:\n",
    "        # Convert relevance judgments to binary values\n",
    "        relevance_vector = [1 if doc in query_judgments_filtered else 0 for doc in query_results_list_filtered]\n",
    "        \n",
    "        # Calculate Precision@M and Recall@M\n",
    "        precision = precision_at_k(relevance_vector, len(query_judgments))\n",
    "        recall = recall_at_k(relevance_vector, len(query_judgments), len(query_judgments))\n",
    "        \n",
    "        # Calculate NDCG@M using non-binary relevance values\n",
    "        ndcg = ndcg_at_k([int(judgment[judgment.find(\":\")+1:]) for judgment in query_judgments], len(query_judgments))\n",
    "        \n",
    "        metrics_by_query[query] = {\"precision\": precision, \"recall\": recall, \"ndcg\": ndcg}\n",
    "    else:\n",
    "        metrics_by_query[query] = {\"precision\": 0.0, \"recall\": 0.0, \"ndcg\": 0.0}\n",
    "\n",
    "# Calculate MAP\n",
    "binary_relevance_vectors = []\n",
    "for query in metrics_by_query:\n",
    "    if query in relevance_judgments:\n",
    "        query_judgments = relevance_judgments[query]\n",
    "        query_judgments_filtered = filter_result_list(query_judgments)\n",
    "        \n",
    "        query_results_list = query_results[query]\n",
    "        query_results_list_filtered = filter_result_list(query_results_list)\n",
    "\n",
    "        # Convert relevance judgments to binary values\n",
    "        relevance_vector = [1 if doc in query_judgments_filtered else 0 for doc in query_results_list_filtered]\n",
    "        binary_relevance_vectors.append(relevance_vector)\n",
    "\n",
    "map_value = mean_average_precision(binary_relevance_vectors)\n",
    "\n",
    "print(\"Metrics by Query:\")\n",
    "for query, metrics in metrics_by_query.items():\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"P@M: {metrics['precision']:.4f}, R@M: {metrics['recall']:.4f}, NDCG@M: {metrics['ndcg']:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"MAP: {map_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anteia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
