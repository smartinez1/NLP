{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1\n",
    "## Punto 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparación de estrategias de motores de búsqueda\n",
    "A continuación, implementará un motor de búsqueda con cuatro estrategias diferentes.\n",
    "1. Búsqueda binaria usando índice invertido (BSII)\n",
    "3. Recuperación ranqueada y vectorización de documentos (RRDV)\n",
    "Debe hacer su propia implementación usando numpy y pandas.\n",
    "\n",
    "Conjunto de datos: hay tres archivos que componen el conjunto de datos: \n",
    "- “Docs raws texts\" contiene 331 documentos en formato NAF (XML; debe usar el título y el \n",
    "contenido para modelar cada documento). \n",
    "- \"Queries raw texts\" contiene 35 consultas. \n",
    "- \"relevance-judgments.tsv\" contiene para cada consulta los documentos considerados relevantes \n",
    "para cada una de las consultas. Estos documentos relevantes fueron construidos manualmente por \n",
    "jueces humanos y sirven como “ground-truth” y evaluación.\n",
    "\n",
    "Pasos de preprocesamiento: para los siguientes puntos, debe preprocesar documentos y consultas \n",
    "mediante tokenización a nivel de palabra, eliminación de palabras vacías, normalización y stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción completada\n"
     ]
    }
   ],
   "source": [
    "# Se extraen los datos de los archivos comprimidos\n",
    "\n",
    "# Specify the paths to the compressed files and the target directory\n",
    "compressed_files = ['docs-raw-texts.zip', 'queries-raw-texts.zip']\n",
    "\n",
    "# Extract files from each compressed file\n",
    "for compressed_file in compressed_files:\n",
    "    with zipfile.ZipFile(compressed_file, 'r') as zip_ref:\n",
    "        folder_name = os.path.splitext(compressed_file)[0]  # Remove the \".zip\" extension\n",
    "        target_folder = os.path.join(folder_name)\n",
    "        \n",
    "        if not os.path.exists(target_folder):\n",
    "            # Create the folder within the target directory\n",
    "            os.mkdir(target_folder)\n",
    "\n",
    "        \n",
    "            # Extract all files to the target folder\n",
    "            zip_ref.extractall(target_folder)\n",
    "\n",
    "print(\"Extracción completada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths a directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios que contienen los archivos necesarios, cambiar acá si es necesario\n",
    "xml_files_directory = 'docs-raw-texts'\n",
    "\n",
    "queries_directory = \"queries-raw-texts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del Índice invertido\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the NLTK stopwords resource\n",
    "nltk.download('stopwords')\n",
    "# NLTK setup\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función de preprocesamiento, se usará para todos los inputs al modelo (queries y documentos)\n",
    "def preprocess_text(text):\n",
    "    text.strip().lower() # normalización del texto, todo en minúscula y se quitan espacios innecesarios.\n",
    "    tokens = tokenizer.tokenize(text) #tokenización por espacio\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words] # eliminación de palabras vacias y stemming\n",
    "    return tokens # retorna lista con el texto tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on',\n",
       " '26',\n",
       " 'juli',\n",
       " '2023',\n",
       " 'coup',\n",
       " 'état',\n",
       " 'occur',\n",
       " 'republ',\n",
       " 'niger',\n",
       " 'countri',\n",
       " 'presidenti',\n",
       " 'guard',\n",
       " 'remov',\n",
       " 'detain',\n",
       " 'presid',\n",
       " 'moham',\n",
       " 'bazoum']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\" On 26 July 2023, a coup d'état ocCuRreD in the Republic of the Niger, in which the country's presidential guard removed...; and detained President Mohamed Bazoum.    \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del diccionario para el índice invertido con base en el corpus de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_text(xml_path: str) -> str:\n",
    "    \"\"\"Extracts raw text from a .naf file.\n",
    "\n",
    "    Args:\n",
    "        xml_path (str): Path to the .naf file.\n",
    "\n",
    "    Returns:\n",
    "        str: The raw text from the .naf file.\n",
    "    \"\"\"\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract content from the XML\n",
    "    raw_text = root.find('raw').text\n",
    "\n",
    "    # Check if the title is present in the raw text\n",
    "    title = root.find(\".//nafHeader/fileDesc\").get(\"title\")\n",
    "    if title and title not in raw_text:\n",
    "        raw_text = title + \", \" + raw_text\n",
    "\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index created.\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the inverted index (term -> list of documents)\n",
    "inverted_index = {}\n",
    "\n",
    "# Dictionary to store term frequencies per document (term -> {document: frequency})\n",
    "term_freq_per_document = {}\n",
    "\n",
    "# Iterate over XML files in the directory\n",
    "for filename in os.listdir(xml_files_directory):\n",
    "    if filename.endswith('.naf'):\n",
    "        xml_path = os.path.join(xml_files_directory, filename)\n",
    "        # Extract content from the XML\n",
    "        content = extract_raw_text(xml_path)\n",
    "        \n",
    "        # Preprocess the content\n",
    "        preprocessed_tokens = preprocess_text(content)\n",
    "        \n",
    "        # Create the inverted index and update term frequencies per document\n",
    "        for term in preprocessed_tokens:\n",
    "            if term in inverted_index:\n",
    "                if filename not in inverted_index[term]:\n",
    "                    inverted_index[term].append(filename)\n",
    "            else:\n",
    "                inverted_index[term] = [filename]\n",
    "            \n",
    "            if term in term_freq_per_document:\n",
    "                if filename in term_freq_per_document[term]:\n",
    "                    term_freq_per_document[term][filename] += 1\n",
    "                else:\n",
    "                    term_freq_per_document[term][filename] = 1\n",
    "            else:\n",
    "                term_freq_per_document[term] = {filename: 1}\n",
    "\n",
    "print(\"Inverted index created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of documents where the term is found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wes2015.d137.naf', 'wes2015.d280.naf']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"list of documents where the term is found\")\n",
    "test_term = preprocess_text(\"confused\")[0]\n",
    "inverted_index[test_term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency value of term:\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"frequency value of term:\")\n",
    "for term in inverted_index[test_term]:\n",
    "    print(term_freq_per_document[test_term][term])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query(inverted_index, query):\n",
    "    # Tokenize the query\n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    # Initialize result set with documents containing the first term\n",
    "    result_set = set(inverted_index.get(query_tokens[0], []))\n",
    "\n",
    "    # Iterate over query tokens and perform AND and NOT operations\n",
    "    operator = None\n",
    "    for token in query_tokens[1:]:\n",
    "        if token == 'and':\n",
    "            operator = 'AND'\n",
    "        elif token == 'not':\n",
    "            operator = 'NOT'\n",
    "        else:\n",
    "            if operator == 'AND':\n",
    "                result_set &= set(inverted_index.get(token, []))\n",
    "            elif operator == 'NOT':\n",
    "                result_set -= set(inverted_index.get(token, []))\n",
    "            else:\n",
    "                result_set |= set(inverted_index.get(token, []))\n",
    "            operator = None\n",
    "    # If the operator is 'AND', the result set is updated using set intersection (&).\n",
    "    # If the operator is 'NOT', the result set is updated using set difference (-).\n",
    "    # If no operator is set, the result set is updated using set union (|).\n",
    "    return list(result_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 result: ['wes2015.d280.naf', 'wes2015.d137.naf']\n"
     ]
    }
   ],
   "source": [
    "query = \"confused\"\n",
    "\n",
    "# Perform queries\n",
    "result = boolean_query(inverted_index, query)\n",
    "print(\"Query 1 result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 result: ['wes2015.d019.naf', 'wes2015.d247.naf', 'wes2015.d316.naf', 'wes2015.d193.naf', 'wes2015.d241.naf', 'wes2015.d190.naf', 'wes2015.d212.naf', 'wes2015.d137.naf', 'wes2015.d102.naf', 'wes2015.d105.naf']\n"
     ]
    }
   ],
   "source": [
    "query = \"Anne\"\n",
    "\n",
    "# Perform queries\n",
    "result = boolean_query(inverted_index, query)\n",
    "print(\"Query 1 result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 result: ['wes2015.d137.naf']\n"
     ]
    }
   ],
   "source": [
    "query = \"confused AND Anne\"\n",
    "\n",
    "# Perform queries\n",
    "result = boolean_query(inverted_index, query)\n",
    "print(\"Query 1 result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 result: ['wes2015.d280.naf']\n"
     ]
    }
   ],
   "source": [
    "query = \"confused NOT Anne\"\n",
    "\n",
    "# Perform queries\n",
    "result = boolean_query(inverted_index, query)\n",
    "print(\"Query 1 result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 result: ['wes2015.d174.naf', 'wes2015.d137.naf']\n"
     ]
    }
   ],
   "source": [
    "query = \"confused OR Anne AND Language NOT william\"\n",
    "\n",
    "# Perform queries\n",
    "result = boolean_query(inverted_index, query)\n",
    "print(\"Query 1 result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consultas binarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_results = {}\n",
    "\n",
    "for filename in os.listdir(queries_directory):\n",
    "    if filename.endswith('.naf'):\n",
    "        query_path = os.path.join(queries_directory, filename)\n",
    "        \n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(query_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Extract query content from the XML\n",
    "        query_id = root.find('nafHeader/public').get('publicId')\n",
    "        query_content = root.find('raw').text\n",
    "        \n",
    "        # Preprocess the query content\n",
    "        preprocessed_query = preprocess_text(query_content)\n",
    "        queries_results[query_id] = preprocessed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform AND queries using the inverted index\n",
    "results = {}\n",
    "for query_id, query_terms in queries_results.items():\n",
    "    result_docs = set(inverted_index.get(query_terms[0], []))\n",
    "    for term in query_terms[1:]:\n",
    "        result_docs &= set(inverted_index.get(term, []))\n",
    "    results[query_id] = result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results written to BSII-AND-queries_results.tsv\n"
     ]
    }
   ],
   "source": [
    "# Write the results to a file\n",
    "output_filename = 'BSII-AND-queries_results.tsv'\n",
    "with open(output_filename, 'w') as output_file:\n",
    "    for query_id, docs in results.items():\n",
    "        doc_numbers = [doc.split('.')[1] for doc in docs]\n",
    "        doc_numbers_str = ','.join(doc_numbers)\n",
    "        output_file.write(f\"{query_id}\\t{doc_numbers_str}\\n\")\n",
    "\n",
    "print(\"Query results written to\", output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anteia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
