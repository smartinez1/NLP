{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USUARIO\\anaconda3\\envs\\anteia\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\USUARIO\\anaconda3\\envs\\anteia\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\USUARIO\\anaconda3\\envs\\anteia\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "import tarfile\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from tqdm import tqdm \n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Download NLTK data for tokenization\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe descaragr el archivo \"Datasets.zip\" en el mismo directorio de el notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción completada\n"
     ]
    }
   ],
   "source": [
    "compressed_file = \"Datasets.zip\"\n",
    "with zipfile.ZipFile(compressed_file, 'r') as zip_ref:\n",
    "    folder_name = os.path.splitext(compressed_file)[0]  # Remove the \".zip\" extension\n",
    "    target_folder = os.path.join(folder_name)\n",
    "    \n",
    "    if not os.path.exists(target_folder):\n",
    "        # Create the folder within the target directory\n",
    "        os.mkdir(target_folder)\n",
    "\n",
    "    \n",
    "        # Extract all files to the target folder\n",
    "        zip_ref.extractall(target_folder)\n",
    "\n",
    "print(\"Extracción completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed\n"
     ]
    }
   ],
   "source": [
    "compressed_file = \"Datasets\\\\20news-18828.tar.gz\"\n",
    "folder_name = os.path.splitext(os.path.splitext(compressed_file)[0])[0]  # Remove the \".tar.gz\" extension\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    os.mkdir(folder_name)\n",
    "\n",
    "# Extract all files from the TAR.GZ archive to the target folder without creating an additional subfolder\n",
    "with tarfile.open(compressed_file, 'r:gz') as tar_ref:\n",
    "    members = tar_ref.getmembers()\n",
    "    tar_ref.extractall(path=folder_name, members=members)\n",
    "\n",
    "print(\"Extracción completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción completada\n"
     ]
    }
   ],
   "source": [
    "compressed_file = \"Datasets\\\\BAC\\\\blogs.zip\"\n",
    "with zipfile.ZipFile(compressed_file, 'r') as zip_ref:\n",
    "    folder_name = os.path.splitext(compressed_file)[0]  # Remove the \".zip\" extension\n",
    "    target_folder = os.path.join(folder_name)\n",
    "    \n",
    "    if not os.path.exists(target_folder):\n",
    "        # Create the folder within the target directory\n",
    "        os.mkdir(target_folder)\n",
    "\n",
    "    \n",
    "        # Extract all files to the target folder\n",
    "        zip_ref.extractall(target_folder)\n",
    "\n",
    "print(\"Extracción completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de Archivos consolidados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents: 0it [00:00, ?it/s]\n",
      "Processing Documents: 0it [00:00, ?it/s]\n",
      "Processing Documents: 100%|██████████| 799/799 [00:03<00:00, 205.13it/s]\n",
      "Processing Documents: 100%|██████████| 973/973 [00:04<00:00, 216.45it/s]\n",
      "Processing Documents: 100%|██████████| 985/985 [00:04<00:00, 219.59it/s]\n",
      "Processing Documents: 100%|██████████| 982/982 [00:04<00:00, 214.97it/s]\n",
      "Processing Documents: 100%|██████████| 961/961 [00:04<00:00, 215.41it/s]\n",
      "Processing Documents: 100%|██████████| 980/980 [00:04<00:00, 210.10it/s]\n",
      "Processing Documents: 100%|██████████| 972/972 [00:03<00:00, 255.44it/s]\n",
      "Processing Documents: 100%|██████████| 990/990 [00:04<00:00, 225.15it/s]\n",
      "Processing Documents: 100%|██████████| 994/994 [00:04<00:00, 238.25it/s]\n",
      "Processing Documents: 100%|██████████| 994/994 [00:04<00:00, 216.18it/s]\n",
      "Processing Documents: 100%|██████████| 999/999 [00:04<00:00, 232.89it/s]\n",
      "Processing Documents: 100%|██████████| 991/991 [00:04<00:00, 229.10it/s]\n",
      "Processing Documents: 100%|██████████| 981/981 [00:03<00:00, 257.04it/s]\n",
      "Processing Documents: 100%|██████████| 990/990 [00:03<00:00, 252.21it/s]\n",
      "Processing Documents: 100%|██████████| 987/987 [00:03<00:00, 250.01it/s]\n",
      "Processing Documents: 100%|██████████| 997/997 [00:04<00:00, 244.67it/s]\n",
      "Processing Documents: 100%|██████████| 910/910 [00:03<00:00, 249.87it/s]\n",
      "Processing Documents: 100%|██████████| 940/940 [00:04<00:00, 216.43it/s]\n",
      "Processing Documents: 100%|██████████| 775/775 [00:03<00:00, 233.87it/s]\n",
      "Processing Documents: 100%|██████████| 628/628 [00:02<00:00, 234.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated 20N dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the paths to the dataset folders\n",
    "path_20N = \"Datasets\\\\20news-18828\"\n",
    "\n",
    "# Define the paths to the consolidated files\n",
    "consolidated_20N_file = \"consolidated_20N.txt\"\n",
    "\n",
    "# Function to read and consolidate documents in a folder\n",
    "def consolidate_documents(folder_path, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file_name in tqdm(files, desc=\"Processing Documents\"):  # Add tqdm here\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "                    content = file.read()\n",
    "                    output.write(content)\n",
    "                    output.write(\"\\n\")  # Add a newline to separate documents\n",
    "\n",
    "# Consolidate documents from the \"20news-18828\" dataset with a progress bar\n",
    "consolidate_documents(path_20N, consolidated_20N_file)\n",
    "print(\"Consolidated 20N dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing XML files: 0it [00:00, ?it/s]\n",
      "Processing XML files: 100%|██████████| 19320/19320 [02:07<00:00, 151.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated BAC dataset (text content from XML documents within 'blogs' folder).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the BAC dataset folder\n",
    "path_BAC = \"Datasets\\\\BAC\"\n",
    "\n",
    "# Define the path to the consolidated text file\n",
    "consolidated_BAC_file = \"consolidated_BAC.txt\"\n",
    "\n",
    "# Function to consolidate text from XML documents in a folder\n",
    "def consolidate_text_from_xml(folder_path, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file_name in tqdm(files, desc=\"Processing XML files\"):  # Add tqdm here\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                # Check if the file is within the \"blogs\" folder and has a \".xml\" extension\n",
    "                if \"blogs\" in root and file_name.endswith(\".xml\"):\n",
    "                    try:\n",
    "                        tree = ET.parse(file_path)\n",
    "                        root_element = tree.getroot()\n",
    "                        for post_element in root_element.findall(\".//post\"):\n",
    "                            content = post_element.text.strip() if post_element.text else \"\"\n",
    "                            # Write the extracted content to the output file\n",
    "                            output.write(content)\n",
    "                            output.write(\"\\n\")  # Add a newline to separate documents\n",
    "                    except ET.ParseError as e:\n",
    "                        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file_:\n",
    "                            xml_content = file_.read()\n",
    "\n",
    "                        # Define a regular expression pattern to match content between <date> and </date> tags\n",
    "                        pattern = r'<date>.*?</date>'\n",
    "\n",
    "                        # Use re.sub() to replace matched patterns with an empty string\n",
    "                        xml_content_filtered = re.sub(pattern, '', xml_content, flags=re.DOTALL)\n",
    "\n",
    "                        xml_content_filtered = re.sub(r'<.*?>', '', xml_content_filtered)\n",
    "                        # Remove multiple spaces with a single space\n",
    "                        xml_content_filtered = re.sub(r'\\s+', ' ', xml_content_filtered)\n",
    "\n",
    "                        xml_content_filtered = xml_content_filtered.replace('\\n', '')\n",
    "                        output.write(xml_content_filtered.strip())\n",
    "                        output.write(\"\\n\")  # Add a newline to separate documents\n",
    "                        # print(f\"Error parsing {file_path}: {e}, using raw file...\")\n",
    "\n",
    "# Path to the \"blogs\" folder within the BAC dataset\n",
    "blogs_path_BAC = os.path.join(path_BAC, \"blogs\")\n",
    "\n",
    "# Consolidate text content from XML documents within the \"blogs\" folder of the BAC dataset\n",
    "consolidate_text_from_xml(blogs_path_BAC, consolidated_BAC_file)\n",
    "print(\"Consolidated BAC dataset (text content from XML documents within 'blogs' folder).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: <s> <UNK> <UNK> <UNK> <UNK> <UNK> . </s>\n",
      "2: <s> <UNK> <UNK> <UNK> <UNK> NUM <UNK> NUM . </s>\n",
      "3: <s> <UNK> <UNK> <UNK> <UNK> <UNK> . </s>\n",
      "4: <s> <UNK> </s>\n"
     ]
    }
   ],
   "source": [
    "# Define a function to preprocess and tokenize text with a stopping parameter\n",
    "def preprocess_and_tokenize(text, max_chars=None):\n",
    "    # Truncate the input text if it exceeds the specified maximum characters\n",
    "    if max_chars is not None and len(text) > max_chars:\n",
    "        text = text[:max_chars]\n",
    "\n",
    "    # Tokenize text by sentence\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Token frequency counter\n",
    "    token_freq = Counter()\n",
    "\n",
    "    # Define a list to store the preprocessed sentences\n",
    "    preprocessed_sentences = []\n",
    "\n",
    "    # Initialize a sentence count\n",
    "    sentence_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Add sentence start and end tags\n",
    "        sentence = \"<s> \" + sentence + \" </s>\"\n",
    "\n",
    "        # Tokenize words within the sentence\n",
    "        words = word_tokenize(sentence)\n",
    "\n",
    "        # Normalize and process each word\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            # Normalize word (convert to lowercase)\n",
    "            word = word.lower().strip()\n",
    "\n",
    "            # Check if the word is a number and replace with \"NUM\"\n",
    "            if re.match(r'^[0-9]+$', word):\n",
    "                word = \"NUM\"\n",
    "\n",
    "            # Update token frequency\n",
    "            token_freq[word] += 1\n",
    "\n",
    "            processed_words.append(word)\n",
    "\n",
    "        # Join the processed words back into a sentence\n",
    "        preprocessed_sentence = \" \".join(processed_words)\n",
    "\n",
    "        # Remove extra spaces between \"<\", \"s\", and \">\"\n",
    "        preprocessed_sentence = preprocessed_sentence.replace(\"< \", \"<\").replace(\" >\", \">\")\n",
    "\n",
    "        # Add the preprocessed sentence to the list\n",
    "        preprocessed_sentences.append(preprocessed_sentence)\n",
    "\n",
    "        # Increment the sentence count\n",
    "        sentence_count += 1\n",
    "\n",
    "    # Replace tokens with unit frequency as \"<UNK>\"\n",
    "    for i in range(len(preprocessed_sentences)):\n",
    "        words = preprocessed_sentences[i].split()\n",
    "        for j in range(len(words)):\n",
    "            if token_freq[words[j]] == 1 and words[j] != \"NUM\":\n",
    "                words[j] = \"<UNK>\"\n",
    "        preprocessed_sentences[i] = \" \".join(words)\n",
    "\n",
    "    # Return the preprocessed sentences\n",
    "    return preprocessed_sentences\n",
    "\n",
    "text = \"This is an example sentence. It contains numbers like 123 and 456789. Some words may be removed. Some words may be removed x 2\"\n",
    "\n",
    "# Preprocess and tokenize the text\n",
    "preprocessed_sentences = preprocess_and_tokenize(text, max_chars=100)\n",
    "\n",
    "# Print the preprocessed sentences\n",
    "for idx, sentence in enumerate(preprocessed_sentences):\n",
    "    print(\"{}: {}\".format(idx+1, sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partición del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing files created for 20N and BAC datasets.\n"
     ]
    }
   ],
   "source": [
    "#maximo numero de caracteres o nunca termina 100.000.000 max\n",
    "max_chars = 100000000\n",
    "# Load the consolidated files\n",
    "with open(\"consolidated_20N.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    sentences_20N = file.read()\n",
    "preprocessed_sentences_20N = preprocess_and_tokenize(sentences_20N, max_chars=max_chars)\n",
    "\n",
    "with open(\"consolidated_BAC.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    sentences_BAC = file.read()\n",
    "preprocessed_sentences_BAC = preprocess_and_tokenize(sentences_BAC, max_chars=max_chars)\n",
    "\n",
    "# Function to split sentences into training and testing sets\n",
    "def split_sentences(sentences, train_ratio):\n",
    "    random.shuffle(sentences)\n",
    "    split_index = int(len(sentences) * train_ratio)\n",
    "    train_sentences = sentences[:split_index]\n",
    "    test_sentences = sentences[split_index:]\n",
    "    return train_sentences, test_sentences\n",
    "\n",
    "# Define the training ratio (80%)\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Split sentences for 20N dataset\n",
    "train_sentences_20N, test_sentences_20N = split_sentences(preprocessed_sentences_20N, train_ratio)\n",
    "\n",
    "# Split sentences for BAC dataset\n",
    "train_sentences_BAC, test_sentences_BAC = split_sentences(preprocessed_sentences_BAC, train_ratio)\n",
    "\n",
    "# Define a group code\n",
    "group_code = \"group_code\" \n",
    "\n",
    "# Create training and testing files\n",
    "with open(f\"20N_{group_code}_training.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(train_sentences_20N)\n",
    "\n",
    "with open(f\"20N_{group_code}_testing.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(test_sentences_20N)\n",
    "\n",
    "with open(f\"BAC_{group_code}_training.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(train_sentences_BAC)\n",
    "\n",
    "with open(f\"BAC_{group_code}_testing.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(test_sentences_BAC)\n",
    "\n",
    "# Print confirmation messages\n",
    "print(\"Training and testing files created for 20N and BAC datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete variables to free up memory\n",
    "del sentences_20N\n",
    "del preprocessed_sentences_20N\n",
    "del sentences_BAC\n",
    "del preprocessed_sentences_BAC\n",
    "del train_sentences_20N\n",
    "del test_sentences_20N\n",
    "del train_sentences_BAC\n",
    "del test_sentences_BAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos N-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'n_grams' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Specify the folder name\n",
    "n_grams_folder = \"n_grams\"\n",
    "\n",
    "# Check if the folder already exists\n",
    "if not os.path.exists(n_grams_folder):\n",
    "    # If it doesn't exist, create it\n",
    "    os.mkdir(n_grams_folder)\n",
    "    print(f\"Folder '{n_grams_folder}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{n_grams_folder}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate Laplace-smoothed probabilities for n-grams\n",
    "def laplace_smoothing(ngram_counts, total_tokens, vocab_size, alpha=1.0):\n",
    "    smoothed_probabilities = {}\n",
    "    for ngram, count in ngram_counts.items():\n",
    "        smoothed_prob = (count + alpha) / (total_tokens + alpha * vocab_size)\n",
    "        smoothed_probabilities[ngram] = smoothed_prob\n",
    "\n",
    "    return smoothed_probabilities\n",
    "\n",
    "# Define a function to generate n-grams from a list of tokens\n",
    "def generate_ngrams(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i + n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Define a function to build and save N-gram models with Laplace smoothing\n",
    "def build_and_save_ngram_model(sentences, n, output_file):\n",
    "    # Tokenize the sentences into words\n",
    "    tokens = sentences.split(\" \")\n",
    "\n",
    "    # Generate n-grams\n",
    "    ngrams = generate_ngrams(tokens, n)\n",
    "\n",
    "    # Calculate n-gram counts\n",
    "    ngram_counts = Counter(ngrams)\n",
    "\n",
    "    # Calculate vocabulary size\n",
    "    vocab_size = len(set(tokens))\n",
    "\n",
    "    # Calculate total number of tokens\n",
    "    total_tokens = len(tokens)\n",
    "\n",
    "    # Apply Laplace smoothing to calculate smoothed probabilities\n",
    "    smoothed_probabilities = laplace_smoothing(ngram_counts, total_tokens, vocab_size)\n",
    "\n",
    "    # Save the smoothed probabilities to the output file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for ngram, prob in smoothed_probabilities.items():\n",
    "            ngram_str = \" \".join(ngram)\n",
    "            file.write(f\"{ngram_str}\\t{prob}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram model 20N\n",
      "2-gram model 20N\n",
      "3-gram model 20N\n",
      "1-gram model BAC\n",
      "2-gram model BAC\n",
      "3-gram model BAC\n",
      "N-gram models with Laplace smoothing generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Paths to preprocessed training files\n",
    "preprocessed_training_file_20N = f\"20N_{group_code}_training.txt\"\n",
    "preprocessed_training_file_BAC = f\"BAC_{group_code}_training.txt\"\n",
    "\n",
    "# Load preprocessed training data from the files\n",
    "with open(preprocessed_training_file_20N, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_sentences_20N = file.read()\n",
    "\n",
    "with open(preprocessed_training_file_BAC, \"r\", encoding=\"utf-8\") as file:\n",
    "    train_sentences_BAC = file.read()\n",
    "\n",
    "# Paths to training files\n",
    "training_file_20N = f\"20N_{group_code}_training.txt\"\n",
    "training_file_BAC = f\"BAC_{group_code}_training.txt\"\n",
    "\n",
    "# Paths to output files\n",
    "output_file_20N_unigrams = f\"20N_{group_code}_unigrams.txt\"\n",
    "output_file_20N_bigrams = f\"20N_{group_code}_bigrams.txt\"\n",
    "output_file_20N_trigrams = f\"20N_{group_code}_trigrams.txt\"\n",
    "output_file_BAC_unigrams = f\"BAC_{group_code}_unigrams.txt\"\n",
    "output_file_BAC_bigrams = f\"BAC_{group_code}_bigrams.txt\"\n",
    "output_file_BAC_trigrams = f\"BAC_{group_code}_trigrams.txt\"\n",
    "\n",
    "# Build and save N-gram models with Laplace smoothing\n",
    "build_and_save_ngram_model(train_sentences_20N, 1, os.path.join(n_grams_folder,output_file_20N_unigrams))\n",
    "print(\"1-gram model 20N\")\n",
    "build_and_save_ngram_model(train_sentences_20N, 2, os.path.join(n_grams_folder,output_file_20N_bigrams))\n",
    "print(\"2-gram model 20N\")\n",
    "build_and_save_ngram_model(train_sentences_20N, 3, os.path.join(n_grams_folder,output_file_20N_trigrams))\n",
    "print(\"3-gram model 20N\")\n",
    "build_and_save_ngram_model(train_sentences_BAC, 1, os.path.join(n_grams_folder,output_file_BAC_unigrams))\n",
    "print(\"1-gram model BAC\")\n",
    "build_and_save_ngram_model(train_sentences_BAC, 2, os.path.join(n_grams_folder,output_file_BAC_bigrams))\n",
    "print(\"2-gram model BAC\")\n",
    "build_and_save_ngram_model(train_sentences_BAC, 3, os.path.join(n_grams_folder,output_file_BAC_trigrams))\n",
    "print(\"3-gram model BAC\")\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"N-gram models with Laplace smoothing generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
