{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "import tarfile\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from tqdm import tqdm \n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "# Download NLTK data for tokenization\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción completada\n"
     ]
    }
   ],
   "source": [
    "compressed_file = \"Datasets.zip\"\n",
    "with zipfile.ZipFile(compressed_file, 'r') as zip_ref:\n",
    "    folder_name = os.path.splitext(compressed_file)[0]  # Remove the \".zip\" extension\n",
    "    target_folder = os.path.join(folder_name)\n",
    "    \n",
    "    if not os.path.exists(target_folder):\n",
    "        # Create the folder within the target directory\n",
    "        os.mkdir(target_folder)\n",
    "\n",
    "    \n",
    "        # Extract all files to the target folder\n",
    "        zip_ref.extractall(target_folder)\n",
    "\n",
    "print(\"Extracción completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed\n"
     ]
    }
   ],
   "source": [
    "compressed_file = \"Datasets\\\\20news-18828.tar.gz\"\n",
    "folder_name = os.path.splitext(os.path.splitext(compressed_file)[0])[0]  # Remove the \".tar.gz\" extension\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    os.mkdir(folder_name)\n",
    "\n",
    "# Extract all files from the TAR.GZ archive to the target folder without creating an additional subfolder\n",
    "with tarfile.open(compressed_file, 'r:gz') as tar_ref:\n",
    "    members = tar_ref.getmembers()\n",
    "    tar_ref.extractall(path=folder_name, members=members)\n",
    "\n",
    "print(\"Extraction completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción completada\n"
     ]
    }
   ],
   "source": [
    "compressed_file = \"Datasets\\\\BAC\\\\blogs.zip\"\n",
    "with zipfile.ZipFile(compressed_file, 'r') as zip_ref:\n",
    "    folder_name = os.path.splitext(compressed_file)[0]  # Remove the \".zip\" extension\n",
    "    target_folder = os.path.join(folder_name)\n",
    "    \n",
    "    if not os.path.exists(target_folder):\n",
    "        # Create the folder within the target directory\n",
    "        os.mkdir(target_folder)\n",
    "\n",
    "    \n",
    "        # Extract all files to the target folder\n",
    "        zip_ref.extractall(target_folder)\n",
    "\n",
    "print(\"Extracción completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents: 0it [00:00, ?it/s]\n",
      "Processing Documents: 0it [00:00, ?it/s]\n",
      "Processing Documents: 100%|██████████| 799/799 [00:00<00:00, 8600.95it/s]\n",
      "Processing Documents: 100%|██████████| 973/973 [00:00<00:00, 8340.74it/s]\n",
      "Processing Documents: 100%|██████████| 985/985 [00:00<00:00, 8392.97it/s]\n",
      "Processing Documents: 100%|██████████| 982/982 [00:00<00:00, 7895.65it/s]\n",
      "Processing Documents: 100%|██████████| 961/961 [00:00<00:00, 8427.07it/s]\n",
      "Processing Documents: 100%|██████████| 980/980 [00:00<00:00, 8070.51it/s]\n",
      "Processing Documents: 100%|██████████| 972/972 [00:00<00:00, 8056.84it/s]\n",
      "Processing Documents: 100%|██████████| 990/990 [00:00<00:00, 8151.01it/s]\n",
      "Processing Documents: 100%|██████████| 994/994 [00:00<00:00, 7830.85it/s]\n",
      "Processing Documents: 100%|██████████| 994/994 [00:00<00:00, 7856.63it/s]\n",
      "Processing Documents: 100%|██████████| 999/999 [00:00<00:00, 7775.00it/s]\n",
      "Processing Documents: 100%|██████████| 991/991 [00:00<00:00, 8273.73it/s]\n",
      "Processing Documents: 100%|██████████| 981/981 [00:00<00:00, 8474.55it/s]\n",
      "Processing Documents: 100%|██████████| 990/990 [00:00<00:00, 8336.08it/s]\n",
      "Processing Documents: 100%|██████████| 987/987 [00:00<00:00, 8310.85it/s]\n",
      "Processing Documents: 100%|██████████| 997/997 [00:00<00:00, 6962.64it/s]\n",
      "Processing Documents: 100%|██████████| 910/910 [00:00<00:00, 7565.63it/s]\n",
      "Processing Documents: 100%|██████████| 940/940 [00:00<00:00, 6225.18it/s]\n",
      "Processing Documents: 100%|██████████| 775/775 [00:00<00:00, 6550.26it/s]\n",
      "Processing Documents: 100%|██████████| 628/628 [00:00<00:00, 8776.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated 20N dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the paths to the dataset folders\n",
    "path_20N = \"Datasets\\\\20news-18828\"\n",
    "\n",
    "# Define the paths to the consolidated files\n",
    "consolidated_20N_file = \"consolidated_20N.txt\"\n",
    "\n",
    "# Function to read and consolidate documents in a folder\n",
    "def consolidate_documents(folder_path, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file_name in tqdm(files, desc=\"Processing Documents\"):  # Add tqdm here\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "                    content = file.read()\n",
    "                    output.write(content)\n",
    "                    output.write(\"\\n\")  # Add a newline to separate documents\n",
    "\n",
    "# Consolidate documents from the \"20news-18828\" dataset with a progress bar\n",
    "consolidate_documents(path_20N, consolidated_20N_file)\n",
    "print(\"Consolidated 20N dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing XML files: 0it [00:00, ?it/s]\n",
      "Processing XML files: 100%|██████████| 19320/19320 [00:47<00:00, 408.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated BAC dataset (text content from XML documents within 'blogs' folder).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the BAC dataset folder\n",
    "path_BAC = \"Datasets\\\\BAC\"\n",
    "\n",
    "# Define the path to the consolidated text file\n",
    "consolidated_BAC_file = \"consolidated_BAC.txt\"\n",
    "\n",
    "# Function to consolidate text from XML documents in a folder\n",
    "def consolidate_text_from_xml(folder_path, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file_name in tqdm(files, desc=\"Processing XML files\"):  # Add tqdm here\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                # Check if the file is within the \"blogs\" folder and has a \".xml\" extension\n",
    "                if \"blogs\" in root and file_name.endswith(\".xml\"):\n",
    "                    try:\n",
    "                        tree = ET.parse(file_path)\n",
    "                        root_element = tree.getroot()\n",
    "                        for post_element in root_element.findall(\".//post\"):\n",
    "                            content = post_element.text.strip() if post_element.text else \"\"\n",
    "                            # Write the extracted content to the output file\n",
    "                            output.write(content)\n",
    "                            output.write(\"\\n\")  # Add a newline to separate documents\n",
    "                    except ET.ParseError as e:\n",
    "                        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file_:\n",
    "                            xml_content = file_.read()\n",
    "\n",
    "                        # Define a regular expression pattern to match content between <date> and </date> tags\n",
    "                        pattern = r'<date>.*?</date>'\n",
    "\n",
    "                        # Use re.sub() to replace matched patterns with an empty string\n",
    "                        xml_content_filtered = re.sub(pattern, '', xml_content, flags=re.DOTALL)\n",
    "\n",
    "                        xml_content_filtered = re.sub(r'<.*?>', '', xml_content_filtered)\n",
    "                        # Remove multiple spaces with a single space\n",
    "                        xml_content_filtered = re.sub(r'\\s+', ' ', xml_content_filtered)\n",
    "\n",
    "                        xml_content_filtered = xml_content_filtered.replace('\\n', '')\n",
    "                        output.write(xml_content_filtered.strip())\n",
    "                        output.write(\"\\n\")  # Add a newline to separate documents\n",
    "                        # print(f\"Error parsing {file_path}: {e}, using raw file...\")\n",
    "\n",
    "# Path to the \"blogs\" folder within the BAC dataset\n",
    "blogs_path_BAC = os.path.join(path_BAC, \"blogs\")\n",
    "\n",
    "# Consolidate text content from XML documents within the \"blogs\" folder of the BAC dataset\n",
    "consolidate_text_from_xml(blogs_path_BAC, consolidated_BAC_file)\n",
    "print(\"Consolidated BAC dataset (text content from XML documents within 'blogs' folder).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
