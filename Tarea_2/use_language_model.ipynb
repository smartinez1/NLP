{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USUARIO\\anaconda3\\envs\\anteia\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\USUARIO\\anaconda3\\envs\\anteia\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\USUARIO\\anaconda3\\envs\\anteia\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams_folder = \"n_grams\"\n",
    "group_code = \"santiagomartinez_camilocastaneda\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    # Use regular expression to split on spaces while preserving '<s>' and '</s>' as separate tokens\n",
    "    tokens = re.split(r'(\\s|<s>|</s>)', text)\n",
    "    \n",
    "    # Remove empty tokens and tokens containing only special characters\n",
    "    tokens = [token for token in tokens if token.strip() and not re.match(r'^[^a-zA-Z0-9]+$', token)]\n",
    "    \n",
    "    # Remove trailing '>' characters from tokens like \"the>\"\n",
    "    tokens = [re.sub(r'(.*[^>])>', r'\\1', token) if token not in ['<s>','</s>', '<UNK>'] else token for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Define a function to calculate perplexity with Laplace smoothing\n",
    "def calculate_perplexity(tokens, model_file, n):\n",
    "    N = len(tokens)\n",
    "    log_prob = 0.0\n",
    "    \n",
    "    with open(model_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        model_data = file.read().splitlines()\n",
    "    \n",
    "    model = {}\n",
    "    for line in model_data:\n",
    "        ngram, probability = line.split('\\t')\n",
    "        model[ngram] = float(probability)\n",
    "    for i in range(1, N):\n",
    "        # Construct the n-gram (unigram, bigram, or trigram)\n",
    "        ngram = \" \".join(tokens[max(0, i - n + 1):i + 1])\n",
    "        # Calculate the conditional probability of the n-gram\n",
    "        prob = model.get(ngram, 0)  # Use Laplace smoothing with a default of 0\n",
    "        \n",
    "        if prob == 0:\n",
    "            # Laplace smoothing: add 1 to the count of unseen n-grams\n",
    "            prob = 1 / (len(model) + 1)\n",
    "        \n",
    "        # Update the log probability\n",
    "        log_prob += np.log(prob)\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity = np.exp(-log_prob / N)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to output files\n",
    "output_file_20N_unigrams = f\"20N_{group_code}_unigrams.txt\"\n",
    "output_file_20N_bigrams = f\"20N_{group_code}_bigrams.txt\"\n",
    "output_file_20N_trigrams = f\"20N_{group_code}_trigrams.txt\"\n",
    "\n",
    "output_file_BAC_unigrams = f\"BAC_{group_code}_unigrams.txt\"\n",
    "output_file_BAC_bigrams = f\"BAC_{group_code}_bigrams.txt\"\n",
    "output_file_BAC_trigrams = f\"BAC_{group_code}_trigrams.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 20N dataset\n",
      "Perplexity for Unigrams Model: 1304.3970894236732\n",
      "Perplexity for Bigrams Model: 77362.21812197147\n",
      "Perplexity for Trigrams Model: 622158.1869173201\n"
     ]
    }
   ],
   "source": [
    "unigrams_model = os.path.join(n_grams_folder, output_file_20N_unigrams)\n",
    "bigrams_model = os.path.join(n_grams_folder, output_file_20N_bigrams)\n",
    "trigrams_model = os.path.join(n_grams_folder, output_file_20N_trigrams)\n",
    "\n",
    "test_dataset = f\"20N_{group_code}_testing.txt\"\n",
    "# Load the test dataset\n",
    "with open(test_dataset, \"r\", encoding=\"utf-8\") as file:\n",
    "    test_data = file.read()\n",
    "\n",
    "# Tokenize the test dataset using your custom tokenizer\n",
    "test_tokens = custom_tokenizer(test_data)\n",
    "\n",
    "# Calculate perplexity for the unigrams, bigrams, and trigrams models\n",
    "perplexity_unigrams = calculate_perplexity(test_tokens, unigrams_model, n=1)\n",
    "perplexity_bigrams = calculate_perplexity(test_tokens, bigrams_model, n=2)\n",
    "perplexity_trigrams = calculate_perplexity(test_tokens, trigrams_model, n=3)\n",
    "\n",
    "# Print the perplexity results\n",
    "print(\"Results for 20N dataset\")\n",
    "print(\"Perplexity for Unigrams Model:\", perplexity_unigrams)\n",
    "print(\"Perplexity for Bigrams Model:\", perplexity_bigrams)\n",
    "print(\"Perplexity for Trigrams Model:\", perplexity_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for BAC dataset\n",
      "Perplexity for Unigrams Model: 766.3209107647922\n",
      "Perplexity for Bigrams Model: 64707.8327112743\n",
      "Perplexity for Trigrams Model: 1041388.7212299752\n"
     ]
    }
   ],
   "source": [
    "unigrams_model = os.path.join(n_grams_folder, output_file_BAC_unigrams)\n",
    "bigrams_model = os.path.join(n_grams_folder, output_file_BAC_bigrams)\n",
    "trigrams_model = os.path.join(n_grams_folder, output_file_BAC_trigrams)\n",
    "\n",
    "test_dataset = f\"BAC_{group_code}_testing.txt\"\n",
    "# Load the test dataset\n",
    "with open(test_dataset, \"r\", encoding=\"utf-8\") as file:\n",
    "    test_data = file.read()\n",
    "\n",
    "# Tokenize the test dataset using your custom tokenizer\n",
    "test_tokens = custom_tokenizer(test_data)\n",
    "\n",
    "# Calculate perplexity for the unigrams, bigrams, and trigrams models\n",
    "perplexity_unigrams = calculate_perplexity(test_tokens, unigrams_model, n=1)\n",
    "perplexity_bigrams = calculate_perplexity(test_tokens, bigrams_model, n=2)\n",
    "perplexity_trigrams = calculate_perplexity(test_tokens, trigrams_model, n=3)\n",
    "\n",
    "# Print the perplexity results\n",
    "print(\"Results for BAC dataset\")\n",
    "print(\"Perplexity for Unigrams Model:\", perplexity_unigrams)\n",
    "print(\"Perplexity for Bigrams Model:\", perplexity_bigrams)\n",
    "print(\"Perplexity for Trigrams Model:\", perplexity_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar Frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the n-gram model\n",
    "def load_ngram_model(model_file):\n",
    "    ngram_model = {}\n",
    "    with open(model_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            ngram, probability = line.strip().split(\"\\t\")\n",
    "            ngram_model[ngram] = float(probability)\n",
    "    return ngram_model\n",
    "\n",
    "# Function to generate the next word based on the n-gram model\n",
    "def generate_next_word(ngram_model, current_words):\n",
    "    candidates = []\n",
    "    probabilities = []\n",
    "    for key in ngram_model.keys():\n",
    "        if key.startswith(current_words):\n",
    "            candidates.append(key.split(\" \")[-1])\n",
    "            probabilities.append(ngram_model.get(key))\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # Normalize the probabilities to ensure they sum to 1\n",
    "    total_probability = sum(probabilities)\n",
    "\n",
    "    probabilities = [prob / total_probability for prob in probabilities]\n",
    "\n",
    "    # Use the calculated and normalized probabilities to randomly select the next word\n",
    "    next_word = np.random.choice(candidates, p=probabilities)\n",
    "\n",
    "    return next_word\n",
    "\n",
    "# Function to generate a sentence using unigrams\n",
    "def generate_sentence_unigram(start_word, unigram_model, max_length=50):\n",
    "    sentence = start_word\n",
    "    while len(sentence) < max_length:\n",
    "        next_word = generate_next_word(unigram_model, sentence[-1])\n",
    "        if next_word is None:\n",
    "            break\n",
    "        \n",
    "        sentence.append(next_word)\n",
    "        \n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "    \n",
    "    return \" \".join(sentence)\n",
    "\n",
    "# Function to generate a sentence\n",
    "def generate_sentence(start_word, ngram_model, n, max_length=50):\n",
    "    if n==1:\n",
    "        sentence = [start_word.lower()]\n",
    "        \n",
    "        while len(sentence) < max_length:\n",
    "            next_word = generate_sentence_unigram(ngram_model, sentence[-1])\n",
    "            if next_word is None:\n",
    "                break\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            \n",
    "            if next_word == \"</s>\":\n",
    "                break\n",
    "        \n",
    "        return \" \".join(sentence)\n",
    "    else:\n",
    "        sentence = [start_word.lower()]\n",
    "        \n",
    "        while len(sentence) < max_length:\n",
    "            current_words = \" \".join(sentence[-(n-1):])\n",
    "            next_word = generate_next_word(ngram_model, current_words)\n",
    "            if next_word is None:\n",
    "                break\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            \n",
    "            if next_word == \"</s>\":\n",
    "                break\n",
    "        \n",
    "        return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence:\n",
      "we a really good quantitative NUM verbal </s>\n"
     ]
    }
   ],
   "source": [
    "# Choose the n-gram model to use (e.g., trigrams)\n",
    "n_gram_model_file = os.path.join(n_grams_folder, f\"BAC_{group_code}_trigrams.txt\")\n",
    "# Load the selected n-gram model\n",
    "ngram_model = load_ngram_model(n_gram_model_file)\n",
    "\n",
    "# Example usage:\n",
    "start_word = \"we\"  # Replace with your desired start word\n",
    "generated_sentence = generate_sentence(start_word, ngram_model, n=3)\n",
    "\n",
    "print(\"Generated Sentence:\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence:\n",
      "we today call for a pronounced effect than for files </s>\n"
     ]
    }
   ],
   "source": [
    "# Choose the n-gram model to use (e.g., trigrams)\n",
    "n_gram_model_file = os.path.join(n_grams_folder, f\"20N_{group_code}_trigrams.txt\")\n",
    "# Load the selected n-gram model\n",
    "ngram_model = load_ngram_model(n_gram_model_file)\n",
    "\n",
    "# Example usage:\n",
    "start_word = \"we\"  # Replace with your desired start word\n",
    "generated_sentence = generate_sentence(start_word, ngram_model, n=3)\n",
    "\n",
    "print(\"Generated Sentence:\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence:\n",
      "we that do n't believe he can just some utility knife from the tanning your love languages do n't move from a it times past eleven 's amp attending a so will to make i at the notion i covers things to go grill and fun use in become a\n"
     ]
    }
   ],
   "source": [
    "# Choose the n-gram model to use (e.g., trigrams)\n",
    "n_gram_model_file = os.path.join(n_grams_folder, f\"BAC_{group_code}_bigrams.txt\")\n",
    "# Load the selected n-gram model\n",
    "ngram_model = load_ngram_model(n_gram_model_file)\n",
    "\n",
    "# Example usage:\n",
    "start_word = \"we\"  # Replace with your desired start word\n",
    "generated_sentence = generate_sentence(start_word, ngram_model, n=2)\n",
    "\n",
    "print(\"Generated Sentence:\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence:\n",
      "we ed dictionary arguments i severed at me </s>\n"
     ]
    }
   ],
   "source": [
    "# Choose the n-gram model to use (e.g., trigrams)\n",
    "n_gram_model_file = os.path.join(n_grams_folder, f\"20N_{group_code}_bigrams.txt\")\n",
    "# Load the selected n-gram model\n",
    "ngram_model = load_ngram_model(n_gram_model_file)\n",
    "\n",
    "# Example usage:\n",
    "start_word = \"we\"  # Replace with your desired start word\n",
    "generated_sentence = generate_sentence(start_word, ngram_model, n=2)\n",
    "\n",
    "print(\"Generated Sentence:\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\USUARIO\\Desktop\\tmp\\2023-2\\Precesamiento de Lenguaje Natural\\NLP\\Tarea_2\\use_language_model.ipynb Cell 14\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Example usage:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m start_word \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwe\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Replace with your desired start word\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m generated_sentence \u001b[39m=\u001b[39m generate_sentence(start_word, ngram_model, n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGenerated Sentence:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(generated_sentence)\n",
      "\u001b[1;32mc:\\Users\\USUARIO\\Desktop\\tmp\\2023-2\\Precesamiento de Lenguaje Natural\\NLP\\Tarea_2\\use_language_model.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m sentence \u001b[39m=\u001b[39m [start_word\u001b[39m.\u001b[39mlower()]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(sentence) \u001b[39m<\u001b[39m max_length:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     next_word \u001b[39m=\u001b[39m generate_sentence_unigram(ngram_model, sentence[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39mif\u001b[39;00m next_word \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\USUARIO\\Desktop\\tmp\\2023-2\\Precesamiento de Lenguaje Natural\\NLP\\Tarea_2\\use_language_model.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_sentence_unigram\u001b[39m(start_word, unigram_model, max_length\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     sentence \u001b[39m=\u001b[39m [start_word\u001b[39m.\u001b[39;49mlower()]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(sentence) \u001b[39m<\u001b[39m max_length:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USUARIO/Desktop/tmp/2023-2/Precesamiento%20de%20Lenguaje%20Natural/NLP/Tarea_2/use_language_model.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         next_word \u001b[39m=\u001b[39m generate_next_word(unigram_model, sentence[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Choose the n-gram model to use (e.g., trigrams)\n",
    "n_gram_model_file = os.path.join(n_grams_folder, f\"BAC_{group_code}_unigrams.txt\")\n",
    "# Load the selected n-gram model\n",
    "ngram_model = load_ngram_model(n_gram_model_file)\n",
    "\n",
    "# Example usage:\n",
    "start_word = \"we\"  # Replace with your desired start word\n",
    "generated_sentence = generate_sentence(start_word, ngram_model, n=1)\n",
    "\n",
    "print(\"Generated Sentence:\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anteia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
