{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing models"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 1,
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USUARIO\\anaconda3\\envs\\anteia\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\USUARIO\\anaconda3\\envs\\anteia\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\USUARIO\\anaconda3\\envs\\anteia\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 2,
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams_folder = \"n_grams\"\n",
    "group_code = \"santiagomartinez_camilocastaneda\" "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 90,
=======
   "execution_count": 3,
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'except' statement on line 32 (2249115571.py, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[90], line 35\u001b[1;36m\u001b[0m\n\u001b[1;33m    log_prob += np.log(prob)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'except' statement on line 32\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(text):\n",
    "    # Use regular expression to split on spaces while preserving '<s>' and '</s>' as separate tokens\n",
    "    tokens = re.split(r'(\\s|<s>|</s>)', text)\n",
    "    \n",
    "    # Remove empty tokens and tokens containing only special characters\n",
    "    tokens = [token for token in tokens if token.strip() and not re.match(r'^[^a-zA-Z0-9]+$', token)]\n",
    "    \n",
    "    # Remove trailing '>' characters from tokens like \"the>\"\n",
    "    tokens = [re.sub(r'(.*[^>])>', r'\\1', token) if token not in ['<s>','</s>', '<UNK>'] else token for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Define a function to calculate perplexity with Laplace smoothing\n",
    "def calculate_perplexity(tokens, model_file, n):\n",
    "    N = len(tokens)\n",
    "    log_prob = 0.0\n",
    "    \n",
    "    with open(model_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        model_data = file.read().splitlines()\n",
    "    \n",
    "    model = {}\n",
    "    for line in model_data:\n",
    "        ngram, probability = line.split('\\t')\n",
    "        model[ngram] = float(probability)\n",
    "    for i in range(1, N):\n",
    "        # Construct the n-gram (unigram, bigram, or trigram)\n",
    "        ngram = \" \".join(tokens[max(0, i - n + 1):i + 1])\n",
    "        \n",
    "        # Calculate the conditional probability of the n-gram\n",
<<<<<<< HEAD
    "        try:\n",
    "            prob = model.get(ngram)\n",
    "        except KeyError:\n",
    "            prob = model.get()\n",
    "    \n",
=======
    "        prob = model.get(ngram, 0)  # Use Laplace smoothing with a default of 0\n",
    "        \n",
    "        if prob == 0:\n",
    "            # Laplace smoothing: add 1 to the count of unseen n-grams\n",
    "            prob = 1 / (len(model) + 1)\n",
    "        \n",
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
    "        # Update the log probability\n",
    "        log_prob += np.log(prob)\n",
    "    \n",
    "    # Calculate perplexity\n",
<<<<<<< HEAD
    "    perplexity = 2**(-log_prob / N)\n",
    "    return perplexity"
=======
    "    perplexity = np.exp(-log_prob / N)\n",
    "    return perplexity\n"
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 87,
=======
   "execution_count": 4,
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to output files\n",
    "output_file_20N_unigrams = f\"20N_{group_code}_unigrams.txt\"\n",
    "output_file_20N_bigrams = f\"20N_{group_code}_bigrams.txt\"\n",
    "output_file_20N_trigrams = f\"20N_{group_code}_trigrams.txt\"\n",
    "\n",
    "output_file_BAC_unigrams = f\"BAC_{group_code}_unigrams.txt\"\n",
    "output_file_BAC_bigrams = f\"BAC_{group_code}_bigrams.txt\"\n",
    "output_file_BAC_trigrams = f\"BAC_{group_code}_trigrams.txt\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 88,
=======
   "execution_count": 5,
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 20N dataset\n",
<<<<<<< HEAD
      "Perplexity for Unigrams Model: 73.10281492426122\n",
      "Perplexity for Bigrams Model: 1038.179142266215\n",
      "Perplexity for Trigrams Model: 3760.719117615324\n"
=======
      "Perplexity for Unigrams Model: 1304.3970894236732\n",
      "Perplexity for Bigrams Model: 77362.21812197147\n",
      "Perplexity for Trigrams Model: 622158.1869173201\n"
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
     ]
    }
   ],
   "source": [
    "unigrams_model = os.path.join(n_grams_folder, output_file_20N_unigrams)\n",
    "bigrams_model = os.path.join(n_grams_folder, output_file_20N_bigrams)\n",
    "trigrams_model = os.path.join(n_grams_folder, output_file_20N_trigrams)\n",
    "\n",
    "test_dataset = f\"20N_{group_code}_testing.txt\"\n",
    "# Load the test dataset\n",
    "with open(test_dataset, \"r\", encoding=\"utf-8\") as file:\n",
    "    test_data = file.read()\n",
    "\n",
    "# Tokenize the test dataset using your custom tokenizer\n",
    "test_tokens = custom_tokenizer(test_data)\n",
    "\n",
    "# Calculate perplexity for the unigrams, bigrams, and trigrams models\n",
    "perplexity_unigrams = calculate_perplexity(test_tokens, unigrams_model, n=1)\n",
    "perplexity_bigrams = calculate_perplexity(test_tokens, bigrams_model, n=2)\n",
    "perplexity_trigrams = calculate_perplexity(test_tokens, trigrams_model, n=3)\n",
    "\n",
    "# Print the perplexity results\n",
    "print(\"Results for 20N dataset\")\n",
    "print(\"Perplexity for Unigrams Model:\", perplexity_unigrams)\n",
    "print(\"Perplexity for Bigrams Model:\", perplexity_bigrams)\n",
    "print(\"Perplexity for Trigrams Model:\", perplexity_trigrams)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 89,
=======
   "execution_count": 6,
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for BAC dataset\n",
<<<<<<< HEAD
      "Perplexity for Unigrams Model: 53.646800510717\n",
      "Perplexity for Bigrams Model: 718.8734518024078\n",
      "Perplexity for Trigrams Model: 3229.8609504650863\n"
=======
      "Perplexity for Unigrams Model: 766.3209107647922\n",
      "Perplexity for Bigrams Model: 64707.8327112743\n",
      "Perplexity for Trigrams Model: 1041388.7212299752\n"
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
     ]
    }
   ],
   "source": [
    "unigrams_model = os.path.join(n_grams_folder, output_file_BAC_unigrams)\n",
    "bigrams_model = os.path.join(n_grams_folder, output_file_BAC_bigrams)\n",
    "trigrams_model = os.path.join(n_grams_folder, output_file_BAC_trigrams)\n",
    "\n",
    "test_dataset = f\"BAC_{group_code}_testing.txt\"\n",
    "# Load the test dataset\n",
    "with open(test_dataset, \"r\", encoding=\"utf-8\") as file:\n",
    "    test_data = file.read()\n",
    "\n",
    "# Tokenize the test dataset using your custom tokenizer\n",
    "test_tokens = custom_tokenizer(test_data)\n",
    "\n",
    "# Calculate perplexity for the unigrams, bigrams, and trigrams models\n",
    "perplexity_unigrams = calculate_perplexity(test_tokens, unigrams_model, n=1)\n",
    "perplexity_bigrams = calculate_perplexity(test_tokens, bigrams_model, n=2)\n",
    "perplexity_trigrams = calculate_perplexity(test_tokens, trigrams_model, n=3)\n",
    "\n",
    "# Print the perplexity results\n",
    "print(\"Results for BAC dataset\")\n",
    "print(\"Perplexity for Unigrams Model:\", perplexity_unigrams)\n",
    "print(\"Perplexity for Bigrams Model:\", perplexity_bigrams)\n",
    "print(\"Perplexity for Trigrams Model:\", perplexity_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar Frases"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 70,
=======
   "execution_count": 40,
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the n-gram model\n",
    "def load_ngram_model(model_file):\n",
    "    ngram_model = {}\n",
    "    with open(model_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            ngram, probability = line.strip().split(\"\\t\")\n",
    "            ngram_model[ngram] = float(probability)\n",
    "    return ngram_model\n",
    "\n",
    "# Function to generate the next word based on the n-gram model\n",
    "def generate_next_word(ngram_model, current_words):\n",
    "    candidates = []\n",
    "    probabilities = []\n",
    "    for key in ngram_model.keys():\n",
    "        if key.startswith(current_words):\n",
    "            candidates.append(key.split(\" \")[-1])\n",
    "            probabilities.append(ngram_model.get(key))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    \n",
    "    # Calculate probabilities for each candidate word\n",
    "    # probabilities = [ngram_model.get(f\"{current_words} {candidate}\", 0.0) for candidate in candidates]\n",
    "\n",
    "    # Normalize the probabilities to ensure they sum to 1\n",
    "    total_probability = sum(probabilities)\n",
    "\n",
    "    probabilities = [prob / total_probability for prob in probabilities]\n",
    "\n",
    "    # Use the calculated and normalized probabilities to randomly select the next word\n",
    "    next_word = np.random.choice(candidates, p=probabilities)\n",
    "\n",
    "    return next_word\n",
    "\n",
    "def generate_next_word_unigram(ngram_model):\n",
    "    candidates = [word for word in ngram_model.keys()]\n",
    "    if not candidates:\n",
    "        return None  # No candidates found, returns random word in vocab\n",
    "\n",
    "    # Calculate probabilities for each candidate word\n",
    "    probabilities = [prob for prob in ngram_model.values()]\n",
    "\n",
    "    # Normalize the probabilities to ensure they sum to 1\n",
    "    total_probability = sum(probabilities)\n",
    "    probabilities = [prob / total_probability for prob in probabilities]\n",
    "\n",
    "    # Use the calculated and normalized probabilities to randomly select the next word\n",
    "    next_word = np.random.choice(candidates, p=probabilities)\n",
    "\n",
    "    return next_word\n",
    "\n",
    "# Function to generate a sentence\n",
    "def generate_sentence(start_word, ngram_model, n, max_length=50):\n",
    "    sentence = [start_word.lower()]\n",
    "    if n==1:\n",
    "        while len(sentence) < max_length:\n",
    "            next_word = generate_next_word_unigram(ngram_model)\n",
    "            sentence.append(next_word)\n",
    "            \n",
    "            if next_word == \"</s>\":\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        while len(sentence) < max_length:\n",
    "            current_words = \" \".join(sentence[-(n-1):])\n",
    "            next_word = generate_next_word(ngram_model, current_words)\n",
    "            if next_word is None:\n",
    "                break\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            \n",
    "            if next_word == \"</s>\":\n",
    "                break\n",
    "        \n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 81,
=======
   "execution_count": 45,
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence:\n",
<<<<<<< HEAD
      "we thought you put it back </s>\n"
=======
      "we to him cos e <UNK> at computer screen by isolating the us but they ship the bow had\n"
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
     ]
    }
   ],
   "source": [
    "# Choose the n-gram model to use (e.g., trigrams)\n",
    "n_gram_model_file = os.path.join(n_grams_folder, f\"BAC_{group_code}_trigrams.txt\")\n",
    "# Load the selected n-gram model\n",
    "ngram_model = load_ngram_model(n_gram_model_file)\n",
    "\n",
    "# Example usage:\n",
    "start_word = \"we\"  # Replace with your desired start word\n",
    "generated_sentence = generate_sentence(start_word, ngram_model, n=3)\n",
    "\n",
    "print(\"Generated Sentence:\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 92,
=======
   "execution_count": 47,
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence:\n",
<<<<<<< HEAD
      "we definitely time in indonesia </s>\n"
=======
      "we do need to have it done </s>\n"
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
     ]
    }
   ],
   "source": [
    "# Choose the n-gram model to use (e.g., trigrams)\n",
    "n_gram_model_file = os.path.join(n_grams_folder, f\"20N_{group_code}_trigrams.txt\")\n",
    "# Load the selected n-gram model\n",
    "ngram_model = load_ngram_model(n_gram_model_file)\n",
    "\n",
    "# Example usage:\n",
    "start_word = \"we\"  # Replace with your desired start word\n",
    "generated_sentence = generate_sentence(start_word, ngram_model, n=3)\n",
    "\n",
    "print(\"Generated Sentence:\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 93,
=======
   "execution_count": 37,
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence:\n",
<<<<<<< HEAD
      "we no know valid\n"
=======
      "we to buy </s>\n"
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
     ]
    }
   ],
   "source": [
    "# Choose the n-gram model to use (e.g., trigrams)\n",
    "n_gram_model_file = os.path.join(n_grams_folder, f\"BAC_{group_code}_bigrams.txt\")\n",
    "# Load the selected n-gram model\n",
    "ngram_model = load_ngram_model(n_gram_model_file)\n",
    "\n",
    "# Example usage:\n",
    "start_word = \"we\"  # Replace with your desired start word\n",
    "generated_sentence = generate_sentence(start_word, ngram_model, n=2)\n",
    "\n",
    "print(\"Generated Sentence:\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 38,
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence:\n",
<<<<<<< HEAD
      "we are often occurs when some honest conclusion </s>\n"
=======
      "we the conflict </s>\n"
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
     ]
    }
   ],
   "source": [
    "# Choose the n-gram model to use (e.g., trigrams)\n",
    "n_gram_model_file = os.path.join(n_grams_folder, f\"20N_{group_code}_bigrams.txt\")\n",
    "# Load the selected n-gram model\n",
    "ngram_model = load_ngram_model(n_gram_model_file)\n",
    "\n",
    "# Example usage:\n",
    "start_word = \"we\"  # Replace with your desired start word\n",
    "generated_sentence = generate_sentence(start_word, ngram_model, n=2)\n",
    "\n",
    "print(\"Generated Sentence:\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 39,
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence:\n",
<<<<<<< HEAD
      "we bit with i yes but with n't the with a to her <s> n't going </s>\n"
=======
      "we a you a it zeal do i finally before bad only knew still i 'm looks is crime less when string it vent 'surreal love get give said guess construe bambi times all back you drug to get of b two the probably the i a great to children\n"
>>>>>>> c6c01563639e1a6c682c76d9419e086aef16ed32
     ]
    }
   ],
   "source": [
    "# Choose the n-gram model to use (e.g., trigrams)\n",
    "n_gram_model_file = os.path.join(n_grams_folder, f\"BAC_{group_code}_unigrams.txt\")\n",
    "# Load the selected n-gram model\n",
    "ngram_model = load_ngram_model(n_gram_model_file)\n",
    "\n",
    "# Example usage:\n",
    "start_word = \"we\"  # Replace with your desired start word\n",
    "generated_sentence = generate_sentence(start_word, ngram_model, n=1)\n",
    "\n",
    "print(\"Generated Sentence:\")\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anteia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
